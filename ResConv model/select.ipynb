{"cells":[{"cell_type":"code","execution_count":108,"metadata":{"execution":{"iopub.execute_input":"2023-03-19T16:28:31.109454Z","iopub.status.busy":"2023-03-19T16:28:31.108450Z","iopub.status.idle":"2023-03-19T16:28:31.121445Z","shell.execute_reply":"2023-03-19T16:28:31.120270Z","shell.execute_reply.started":"2023-03-19T16:28:31.109398Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import torch\n","import torchvision\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","from torch.utils import data\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from torch.utils.data import DataLoader\n","from torch.utils.data import random_split\n","from torch.optim import lr_scheduler\n","from torch.optim import lr_scheduler\n","import time\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","from torch.utils.data import random_split\n","\n","global cr\n","cr = 0.96\n","\n","\n","def normalization(x: list):\n","    M, m = np.max(x), np.min(x)\n","    for i in range(len(x)):\n","        x[i] = (x[i] - (M + m) / 2) / ((M - m) / 2)\n","    # x in [-1, 1]\n","    return M, m, x\n","\n","def ArrNorm(x: np.ndarray):\n","    assert isinstance(x, np.ndarray), \"We need a list\"\n","    M_list, m_list, res = [], [], []\n","    for i in range(x.shape[0]):\n","        u = x[i].tolist()\n","        M, m, t = normalization(u)\n","        res.append(t)\n","        M_list.append(M)\n","        m_list.append(m)\n","    return M_list, m_list, np.array(res)\n","\n","\n","def df2arr(x) -> np.ndarray:\n","    return np.array(x, dtype=np.float32)\n","\n","\n","\n","# one train demo\n","\n","class Try(nn.Module):\n","    def __init__(self, seq, batch_size, scale=0):\n","        super(Try, self).__init__()\n","        self.scale = scale\n","        self.seq = seq\n","        self.batch_size = batch_size\n","        self.linear = nn.Sequential(\n","            nn.Linear((self.seq+1)*12, (self.seq+1)*6),\n","            nn.Dropout(0.5),\n","            nn.Sigmoid(),\n","\n","            nn.Linear((self.seq+1)*6, (self.seq+1)*6),\n","            nn.Dropout(0.5),\n","            nn.ReLU(inplace=True),\n","\n","        )\n","        self.conv1 = nn.Sequential(\n","            # seq * 5 \n","            nn.Conv2d(1, 2, kernel_size=(3,3), padding=2, bias=False), # (seq+2) * 7\n","            nn.BatchNorm2d(2),\n","            nn.ReLU(inplace=True),\n","\n","            nn.Conv2d(2, 2, kernel_size=(3,3), padding=1, bias=False), # (seq+2) * 7\n","            nn.BatchNorm2d(2),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=(2,2), stride=1), # (seq+1) * 6\n","            \n","        )\n","        \n","        self.conv2 = nn.Sequential(\n","            nn.Conv2d(1, 30, kernel_size=(2,2), padding=0, bias=False), # seq * 5\n","            nn.BatchNorm2d(30),\n","            nn.ReLU(inplace=True),\n","\n","            nn.Conv2d(30, 1, kernel_size=(1,1), padding=0, bias=True), # seq * 5\n","            nn.ReLU(inplace=True)\n","\n","        )\n","\n","    def forward(self, x):\n","        out = self.conv1(x)\n","        out = out.view(out.size()[0], -1)\n","        out = self.linear(out)\n","        # print(out.shape)\n","        with torch.no_grad():\n","            out = out.reshape(self.batch_size, 1, self.seq+1, 6)\n","        out = self.conv2(out)\n","        assert out.shape==x.shape, \"Shape Unequal Error.\"\n","        return out + x\n","    \n","    \n","\n","import math\n","import numpy as np\n","from scipy import stats\n","def rsquared(x, y): \n","    assert x.shape==y.shape, \"Unequal Shape Error\"\n","    r, x, y = [], x.detach(), y.detach()\n","    x, y = x.mean(dim=[0,1], keepdim=False), y.mean(dim=[0,1], keepdim=False)\n","    for i in range(x.shape[-1]):\n","        _, _, r_value, _, _ = stats.linregress(x[:][i].detach().numpy(), y[:][i].detach().numpy()) \n","        r.append(r_value ** 2)\n","    return r\n"]},{"cell_type":"code","execution_count":109,"metadata":{"execution":{"iopub.execute_input":"2023-03-19T16:28:31.124287Z","iopub.status.busy":"2023-03-19T16:28:31.123851Z","iopub.status.idle":"2023-03-19T16:28:31.777993Z","shell.execute_reply":"2023-03-19T16:28:31.777028Z","shell.execute_reply.started":"2023-03-19T16:28:31.124240Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(4459, 18)"]},"execution_count":109,"metadata":{},"output_type":"execute_result"}],"source":["excel = pd.read_excel('/kaggle/input/a32-data/A32.xlsx', header=None)\n","excel.shape"]},{"cell_type":"code","execution_count":110,"metadata":{"execution":{"iopub.execute_input":"2023-03-19T16:28:31.781701Z","iopub.status.busy":"2023-03-19T16:28:31.781410Z","iopub.status.idle":"2023-03-19T16:28:31.790090Z","shell.execute_reply":"2023-03-19T16:28:31.788849Z","shell.execute_reply.started":"2023-03-19T16:28:31.781674Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(1486, 5)"]},"execution_count":110,"metadata":{},"output_type":"execute_result"}],"source":["sp = [1486, 2972, 4458]\n","station_1 = excel.iloc[1:sp[0]+1,1:6]\n","station_2 = excel.iloc[sp[0]+1:sp[1]+1,1:6]\n","standard = excel.iloc[sp[1]+1:sp[2]+1,1:6]\n","standard.shape"]},{"cell_type":"code","execution_count":111,"metadata":{"execution":{"iopub.execute_input":"2023-03-19T16:28:31.793514Z","iopub.status.busy":"2023-03-19T16:28:31.792802Z","iopub.status.idle":"2023-03-19T16:28:31.804432Z","shell.execute_reply":"2023-03-19T16:28:31.803164Z","shell.execute_reply.started":"2023-03-19T16:28:31.793476Z"},"trusted":true},"outputs":[{"data":{"text/plain":["((1486, 5), (1486, 5), (1486, 5))"]},"execution_count":111,"metadata":{},"output_type":"execute_result"}],"source":["station_1 = df2arr(station_1)\n","station_2 = df2arr(station_2)\n","standard = df2arr(standard)\n","station_1.shape, station_2.shape, standard.shape"]},{"cell_type":"code","execution_count":112,"metadata":{"execution":{"iopub.execute_input":"2023-03-19T16:28:31.806567Z","iopub.status.busy":"2023-03-19T16:28:31.806025Z","iopub.status.idle":"2023-03-19T16:28:31.815971Z","shell.execute_reply":"2023-03-19T16:28:31.814648Z","shell.execute_reply.started":"2023-03-19T16:28:31.806530Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:3: RuntimeWarning: divide by zero encountered in true_divide\n","  This is separate from the ipykernel package so we can avoid doing imports until\n","/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in true_divide\n","  after removing the cwd from sys.path.\n"]}],"source":["s1_minus_sd = station_1 - standard\n","s2_minus_sd = station_2 - standard\n","s1_div_sd = station_1 / standard\n","s2_div_sd = station_2 / standard"]},{"cell_type":"code","execution_count":113,"metadata":{"execution":{"iopub.execute_input":"2023-03-19T16:28:31.818751Z","iopub.status.busy":"2023-03-19T16:28:31.817747Z","iopub.status.idle":"2023-03-19T16:28:31.825820Z","shell.execute_reply":"2023-03-19T16:28:31.824575Z","shell.execute_reply.started":"2023-03-19T16:28:31.818662Z"},"trusted":true},"outputs":[{"data":{"text/plain":["((1486, 5), (1486, 5))"]},"execution_count":113,"metadata":{},"output_type":"execute_result"}],"source":["s1_minus_sd.shape, s2_minus_sd.shape"]},{"cell_type":"code","execution_count":114,"metadata":{"execution":{"iopub.execute_input":"2023-03-19T16:28:31.828413Z","iopub.status.busy":"2023-03-19T16:28:31.827577Z","iopub.status.idle":"2023-03-19T16:28:31.893468Z","shell.execute_reply":"2023-03-19T16:28:31.892506Z","shell.execute_reply.started":"2023-03-19T16:28:31.828375Z"},"trusted":true},"outputs":[],"source":["s1_M, s1_m, s1 = ArrNorm(station_1)\n","s2_M, s2_m, s2 = ArrNorm(station_2)"]},{"cell_type":"code","execution_count":115,"metadata":{"execution":{"iopub.execute_input":"2023-03-19T16:28:31.896493Z","iopub.status.busy":"2023-03-19T16:28:31.895850Z","iopub.status.idle":"2023-03-19T16:28:31.908466Z","shell.execute_reply":"2023-03-19T16:28:31.907228Z","shell.execute_reply.started":"2023-03-19T16:28:31.896457Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(1486, 5)"]},"execution_count":115,"metadata":{},"output_type":"execute_result"}],"source":["def GetDataset(input_arr: list, output_arr: list, seq: int):\n","    assert(len(input_arr)==len(output_arr)), \"Different size of input and output!\"\n","    Input = []\n","    Output = []\n","    for i in range(input_arr.shape[0]-seq):\n","        Input.append(input_arr[i:i+seq][:])\n","        Output.append(output_arr[i:i+seq][:])\n","    return torch.tensor(Input, dtype=torch.float32), torch.tensor(Output, dtype=torch.float32)\n","\n","        \n","def load_array(data_arrays, batch_size, is_train=True):\n","    # data-iter\n","    dataset = data.TensorDataset(*data_arrays)\n","    return data.DataLoader(dataset, batch_size, shuffle=is_train)\n","\n","s1_minus_sd.shape"]},{"cell_type":"code","execution_count":116,"metadata":{"execution":{"iopub.execute_input":"2023-03-19T16:28:31.914234Z","iopub.status.busy":"2023-03-19T16:28:31.913739Z","iopub.status.idle":"2023-03-19T16:28:31.978653Z","shell.execute_reply":"2023-03-19T16:28:31.977510Z","shell.execute_reply.started":"2023-03-19T16:28:31.914166Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(torch.Size([1480, 6, 5]), torch.Size([1480, 6, 5]))"]},"execution_count":116,"metadata":{},"output_type":"execute_result"}],"source":["# global sequence, batch_size\n","# batch_size = 4\n","# sequence = 6\n","\n","# global lr, num_epoch\n","# lr, num_epoch = 0.000001, 100\n","status_list = ['MINUS', 'DIVIDE']\n","global record\n","record_R = [] * 5\n","record_con = [] * 5\n","\n","def train(status, batch_size, sequence, lr, num_epoch, ga):\n","    string = \"&\"+str(status)+\"&batch_size=\"+str(batch_size)+\"&sequence=\"+str(sequence)+\"&lr=\"+str(lr)+\"&num_epoch\"+str(num_epoch)+\"&gama=\"+str(ga)+\"&\"\n","    \n","    Input_Data_1, Output_Data_1 = GetDataset(s1, s1_minus_sd, sequence) if status==\"MINUS\" else GetDataset(s1,s1_div_sd)\n","    Input_Data_2, Output_Data_2 = GetDataset(s2, s2_minus_sd, sequence) if status==\"MINUS\" else GetDataset(s2,s2_div_sd)\n","    Input_Data_1.shape, Input_Data_2.shape\n","\n","    Input_Data_1 = Input_Data_1.unsqueeze(1)\n","    Output_Data_1 = Output_Data_1.unsqueeze(1)\n","    Input_Data_2 = Input_Data_2.unsqueeze(1)\n","    Output_Data_2 = Output_Data_2.unsqueeze(1)\n","    Input_Data_1.shape, Input_Data_1.shape\n","\n","\n","    data_tot_1 = torch.utils.data.TensorDataset(Input_Data_1, Output_Data_1)\n","    data_tot_2 = torch.utils.data.TensorDataset(Input_Data_2, Output_Data_2)\n","    train_size = int(Input_Data_1.shape[0] * cr)\n","    test_size = Input_Data_1.shape[0] - train_size\n","    train_set_1, test_set_1 = random_split(data_tot_1,[train_size,test_size],\n","                                        torch.Generator().manual_seed(0))\n","    train_set_2, test_set_2 = random_split(data_tot_2,[train_size,test_size],\n","                                        torch.Generator().manual_seed(0))\n","    # DataIter = load_array((Input_Data_1, Output_Data_1), batch_size=8)\n","\n","\n","\n","\n","    Data_Iter_1 = DataLoader(dataset=train_set_1, batch_size=batch_size, shuffle=True, drop_last=True)\n","    Data_Iter_2 = DataLoader(dataset=train_set_2, batch_size=batch_size, shuffle=True, drop_last=True)\n","\n","    for i, dt in enumerate(Data_Iter_1):\n","        if dt[0].shape[0]!=batch_size:\n","            print(dt[0].shape)\n","            print(i, batch_size, dt[0].shape[0], dt)\n","\n","    for i, dt in enumerate(Data_Iter_2):\n","        if dt[0].shape[0]!=batch_size:\n","            print(dt[0].shape)\n","            print(i, batch_size, dt[0].shape[0], dt)\n","\n","    x_plt, train_loss_plt = [], []\n","\n","\n","    net = Try(batch_size=batch_size, seq=sequence).cuda()\n","    Loss = torch.nn.MSELoss()\n","    optimizer = torch.optim.Adam(net.parameters(), lr)\n","    scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=ga)\n","\n","    def Iter(num_epoch):\n","        cnt = 0\n","        while cnt < num_epoch:\n","            yield cnt\n","            cnt += 1\n","\n","    print(\"\\nStart Training with condition: \"+string)\n","    print(\"Round 1...\")\n","    for epoch in range(num_epoch):\n","        epoch_start_time = time.time()\n","        train_loss = 0.0\n","        net.train()\n","        for i, use in enumerate(tqdm(Data_Iter_1)):\n","            optimizer.zero_grad()\n","            # if use[0].shape[0]==2:\n","            #     print(use[0])\n","            train_pred = net(use[0].cuda())    # use[0].cuda()\n","\n","            batch_loss = Loss(train_pred, use[1].cuda())   # use[1].cuda()\n","            batch_loss.backward()\n","            optimizer.step()\n","            # R2 = R_square(train_pred.cpu(), use[1].cpu())\n","\n","            train_loss += batch_loss.item()\n","\n","        train_loss = train_loss / train_size\n","        x_plt.append(epoch+1)\n","        train_loss_plt.append(train_loss)\n","        print(\"Round-1 --- [%2d|%2d] %.2f(s) Train_Loss=%.6f \"%\\\n","                (epoch+1,num_epoch,time.time()-epoch_start_time,train_loss),end='')\n","    #   epoch_start_time = time.time()\n","        scheduler.step()  \n","\n","    plt.figure(1)\n","    plt.plot(x_plt,train_loss_plt,'rs-',label='all_train_loss')\n","    plt.show()\n","\n","    print(\"Round 2...\")\n","    for epoch in range(num_epoch):\n","        epoch_start_time = time.time()\n","        train_loss = 0.0\n","        net.train()\n","        for i, use in enumerate(tqdm(Data_Iter_2)):\n","            optimizer.zero_grad()\n","            # if use[0].shape[0]==2:\n","            #     print(use[0])\n","            train_pred = net(use[0].cuda())    # use[0].cuda()\n","\n","            batch_loss = Loss(train_pred, use[1].cuda())   # use[1].cuda()\n","            batch_loss.backward()\n","            optimizer.step()\n","            # R2 = rsquared(train_pred.cpu(), use[1].cpu())\n","\n","            train_loss += batch_loss.item()\n","\n","        train_loss = train_loss / train_size\n","        x_plt.append(epoch+num_epoch+1)\n","        train_loss_plt.append(train_loss)\n","        print(\"Round 2 --- [%2d|%2d] %.2f(s) Train_Loss=%.6f \"%\\\n","                (epoch+1,num_epoch,time.time()-epoch_start_time,train_loss),end='')\n","    #   epoch_start_time = time.time()\n","        scheduler.step() \n","\n","\n","    plt.figure(2)\n","    plt.plot(x_plt,train_loss_plt,'rs-',label='all_train_loss')\n","    plt.show()\n","\n","    torch.save(net.state_dict(), '/kaggle/working/'+string+\".pt\")\n","    print(\"Parameters Saved.\")\n","\n","\n","    Test_Iter_1 = DataLoader(dataset=test_set_1, batch_size=1, shuffle=False, drop_last=True)\n","    model = Try(batch_size=1, seq=sequence)\n","    model.load_state_dict(torch.load('/kaggle/working/'+string+\".pt\"))\n","    model = model.cuda()\n","    model.eval()\n","    R_list = [] * 5\n","    def app(R: list)-> list:\n","        assert len(R)==5\n","        for i in range(R_list):\n","            R_list[i].append(R[i])\n","\n","    with torch.no_grad():\n","        for i, use in enumerate(Test_Iter_1):\n","            pred = model(use[0].cuda())\n","            R = rsquared(pred.cpu(), use[1])\n","            print(i, R)\n","            app(R)\n","\n","    Test_Iter_2 = DataLoader(dataset=test_set_2, batch_size=1, shuffle=False, drop_last=True)\n","\n","    with torch.no_grad():\n","        for i, use in enumerate(Test_Iter_2):\n","            pred = model(use[0].cuda())\n","    #         print(pred.shape, use[1].shape)\n","            R = rsquared(pred.cpu(), use[1])\n","            print(i, R)\n","            app(R)\n","    R = torch.Tensor(R_list)\n","    R = R.mean(dim=0, keepdim=False)\n","    R = R.tolist()\n","    \n","    record_R.append(R)\n","    record_con.append(R)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for status in status_list:\n","    for num_epoch in [500,1000,1500]:\n","        for ga in range(0,0.9,0.1):\n","            for batch_size in [2,4,8,16,32,64]:\n","                for lr in [0.00001,0.0001,0.0001,0.001,0.03]:\n","                    for sequence in [4,5,6,7,8,9,10]:\n","                        train(status, batch_size, sequence, lr, num_epoch, ga)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}

{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-03-16T02:38:15.138978Z","iopub.status.busy":"2023-03-16T02:38:15.138538Z","iopub.status.idle":"2023-03-16T02:38:19.717135Z","shell.execute_reply":"2023-03-16T02:38:19.716097Z","shell.execute_reply.started":"2023-03-16T02:38:15.138944Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import torch\n","import torchvision\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","from torch.utils import data\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from torch.utils.data import DataLoader\n","from torch.utils.data import random_split\n","from torch.optim import lr_scheduler\n","from torch.optim import lr_scheduler\n","import time\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","\n","def normalization(x: list):\n","    M, m = np.max(x), np.min(x)\n","    for i in range(len(x)):\n","        x[i] = (x[i] - (M + m) / 2) / ((M - m) / 2)\n","    # x in [-1, 1]\n","    return M, m, x\n","\n","def ArrNorm(x: np.ndarray):\n","    assert isinstance(x, np.ndarray), \"We need a list\"\n","    M_list, m_list, res = [], [], []\n","    for i in range(x.shape[0]):\n","        u = x[i].tolist()\n","        M, m, t = normalization(u)\n","        res.append(t)\n","        M_list.append(M)\n","        m_list.append(m)\n","    return M_list, m_list, np.array(res)\n","\n","\n","def df2arr(x) -> np.ndarray:\n","    return np.array(x, dtype=np.float32)\n"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-03-16T02:38:19.719476Z","iopub.status.busy":"2023-03-16T02:38:19.718948Z","iopub.status.idle":"2023-03-16T02:38:20.930066Z","shell.execute_reply":"2023-03-16T02:38:20.928932Z","shell.execute_reply.started":"2023-03-16T02:38:19.719443Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(4459, 18)"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["excel = pd.read_excel('./data/A32.xlsx', header=None)\n","excel.shape"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-03-16T02:38:20.933593Z","iopub.status.busy":"2023-03-16T02:38:20.932198Z","iopub.status.idle":"2023-03-16T02:38:20.943881Z","shell.execute_reply":"2023-03-16T02:38:20.942884Z","shell.execute_reply.started":"2023-03-16T02:38:20.933546Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(1486, 5)"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["sp = [1486, 2972, 4458]\n","station_1 = excel.iloc[1:sp[0]+1,1:6]\n","station_2 = excel.iloc[sp[0]+1:sp[1]+1,1:6]\n","standard = excel.iloc[sp[1]+1:sp[2]+1,1:6]\n","standard.shape"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-03-16T02:38:20.949828Z","iopub.status.busy":"2023-03-16T02:38:20.949461Z","iopub.status.idle":"2023-03-16T02:38:20.960802Z","shell.execute_reply":"2023-03-16T02:38:20.959621Z","shell.execute_reply.started":"2023-03-16T02:38:20.949798Z"},"trusted":true},"outputs":[{"data":{"text/plain":["((1486, 5), (1486, 5), (1486, 5))"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["station_1 = df2arr(station_1)\n","station_2 = df2arr(station_2)\n","standard = df2arr(standard)\n","station_1.shape, station_2.shape, standard.shape"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-03-16T02:38:20.962538Z","iopub.status.busy":"2023-03-16T02:38:20.962171Z","iopub.status.idle":"2023-03-16T02:38:20.973406Z","shell.execute_reply":"2023-03-16T02:38:20.971965Z","shell.execute_reply.started":"2023-03-16T02:38:20.962508Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\Chen Ding\\AppData\\Local\\Temp\\ipykernel_36112\\711113420.py:3: RuntimeWarning: divide by zero encountered in divide\n","  s1_div_sd = station_1 / standard\n","C:\\Users\\Chen Ding\\AppData\\Local\\Temp\\ipykernel_36112\\711113420.py:4: RuntimeWarning: divide by zero encountered in divide\n","  s2_dic_sd = station_2 / standard\n"]}],"source":["s1_minus_sd = station_1 - standard\n","s2_minus_sd = station_2 - standard\n","s1_div_sd = station_1 / standard\n","s2_dic_sd = station_2 / standard"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-03-16T02:38:20.975927Z","iopub.status.busy":"2023-03-16T02:38:20.975467Z","iopub.status.idle":"2023-03-16T02:38:20.986503Z","shell.execute_reply":"2023-03-16T02:38:20.985139Z","shell.execute_reply.started":"2023-03-16T02:38:20.975887Z"},"trusted":true},"outputs":[{"data":{"text/plain":["((1486, 5), (1486, 5))"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["s1_minus_sd.shape, s2_minus_sd.shape"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-03-16T02:38:20.989213Z","iopub.status.busy":"2023-03-16T02:38:20.988790Z","iopub.status.idle":"2023-03-16T02:38:21.050295Z","shell.execute_reply":"2023-03-16T02:38:21.049357Z","shell.execute_reply.started":"2023-03-16T02:38:20.989155Z"},"trusted":true},"outputs":[],"source":["s1_M, s1_m, s1 = ArrNorm(station_1)\n","s2_M, s2_m, s2 = ArrNorm(station_2)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-03-16T02:38:21.052743Z","iopub.status.busy":"2023-03-16T02:38:21.052024Z","iopub.status.idle":"2023-03-16T02:38:21.064769Z","shell.execute_reply":"2023-03-16T02:38:21.063595Z","shell.execute_reply.started":"2023-03-16T02:38:21.052706Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(1486, 5)"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["def GetDataset(input_arr: list, output_arr: list, seq: int):\n","    assert(len(input_arr)==len(output_arr)), \"Different size of input and output!\"\n","    Input = []\n","    Output = []\n","    for i in range(input_arr.shape[0]-seq):\n","        Input.append(input_arr[i:i+seq][:])\n","        Output.append(output_arr[i:i+seq][:])\n","    return torch.tensor(Input, dtype=torch.float32), torch.tensor(Output, dtype=torch.float32)\n","\n","        \n","def load_array(data_arrays, batch_size, is_train=True):\n","    # data-iter\n","    dataset = data.TensorDataset(*data_arrays)\n","    return data.DataLoader(dataset, batch_size, shuffle=is_train)\n","\n","s1_minus_sd.shape"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-03-16T02:38:21.068818Z","iopub.status.busy":"2023-03-16T02:38:21.068284Z","iopub.status.idle":"2023-03-16T02:38:21.247957Z","shell.execute_reply":"2023-03-16T02:38:21.246864Z","shell.execute_reply.started":"2023-03-16T02:38:21.068775Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\Chen Ding\\AppData\\Local\\Temp\\ipykernel_36112\\2906075547.py:8: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:204.)\n","  return torch.tensor(Input, dtype=torch.float32), torch.tensor(Output, dtype=torch.float32)\n"]},{"data":{"text/plain":["(torch.Size([1471, 15, 5]), torch.Size([1471, 15, 5]))"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["global sequence\n","sequence = 15\n","\n","Input_Data_1, Output_Data_1 = GetDataset(s1, s1_minus_sd, sequence)\n","Input_Data_2, Output_Data_2 = GetDataset(s2, s2_minus_sd, sequence)\n","Input_Data_1.shape, Input_Data_2.shape"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-03-16T02:38:21.252491Z","iopub.status.busy":"2023-03-16T02:38:21.252139Z","iopub.status.idle":"2023-03-16T02:38:21.274544Z","shell.execute_reply":"2023-03-16T02:38:21.273268Z","shell.execute_reply.started":"2023-03-16T02:38:21.252460Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(torch.Size([1471, 1, 15, 5]), torch.Size([1471, 1, 15, 5]))"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["Input_Data_1 = Input_Data_1.unsqueeze(1)\n","Output_Data_1 = Output_Data_1.unsqueeze(1)\n","Input_Data_2 = Input_Data_2.unsqueeze(1)\n","Output_Data_2 = Output_Data_2.unsqueeze(1)\n","Input_Data_1.shape, Input_Data_1.shape"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-03-16T02:38:21.276861Z","iopub.status.busy":"2023-03-16T02:38:21.276382Z","iopub.status.idle":"2023-03-16T02:38:21.311232Z","shell.execute_reply":"2023-03-16T02:38:21.310255Z","shell.execute_reply.started":"2023-03-16T02:38:21.276821Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(torch.Size([2942, 1, 15, 5]), torch.Size([2942, 1, 15, 5]))"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["Input_Data = torch.cat((Input_Data_1, Input_Data_2), dim = 0)\n","Output_Data = torch.cat((Output_Data_1, Output_Data_2), dim = 0)\n","Input_Data.shape, Output_Data.shape"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-03-16T02:38:21.313233Z","iopub.status.busy":"2023-03-16T02:38:21.312836Z","iopub.status.idle":"2023-03-16T02:38:21.331934Z","shell.execute_reply":"2023-03-16T02:38:21.330954Z","shell.execute_reply.started":"2023-03-16T02:38:21.313172Z"},"trusted":true},"outputs":[],"source":["from torch.utils.data import random_split\n","\n","global cr\n","cr = 0.85\n","\n","data_tot_1 = torch.utils.data.TensorDataset(Input_Data, Output_Data)\n","train_size = int(Input_Data.shape[0] * cr)\n","test_size = Input_Data.shape[0] - train_size\n","train_set , test_set = random_split(data_tot_1,[train_size,test_size],\n","                                   torch.Generator().manual_seed(0))\n","# DataIter = load_array((Input_Data_1, Output_Data_1), batch_size=8)\n"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-03-16T02:38:21.334151Z","iopub.status.busy":"2023-03-16T02:38:21.333702Z","iopub.status.idle":"2023-03-16T02:38:21.396616Z","shell.execute_reply":"2023-03-16T02:38:21.395567Z","shell.execute_reply.started":"2023-03-16T02:38:21.334108Z"},"trusted":true},"outputs":[],"source":["global batch_size\n","batch_size = 8\n","Data_Iter = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True, drop_last=True)\n","\n","for i, dt in enumerate(Data_Iter):\n","    if dt[0].shape[0]!=batch_size:\n","        print(dt[0].shape)\n","        print(i, batch_size, dt[0].shape[0], dt)\n","\n"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-03-16T02:38:21.398473Z","iopub.status.busy":"2023-03-16T02:38:21.398073Z","iopub.status.idle":"2023-03-16T02:38:21.410563Z","shell.execute_reply":"2023-03-16T02:38:21.409247Z","shell.execute_reply.started":"2023-03-16T02:38:21.398413Z"},"trusted":true},"outputs":[],"source":["# one train demo\n","\n","class Try(nn.Module):\n","    def __init__(self, seq, batch_size, scale=0):\n","        super(Try, self).__init__()\n","        self.scale = scale\n","        self.seq = seq\n","        self.batch_size = batch_size\n","        self.linear = nn.Sequential(\n","            nn.Linear((self.seq+1)*6*5, (self.seq+1)*10),\n","            nn.Dropout(0.5),\n","            nn.Sigmoid(),\n","\n","            nn.Linear((self.seq+1)*10, (self.seq+1)*6),\n","            nn.Dropout(0.5),\n","            nn.ReLU(inplace=True),\n","\n","        )\n","        self.conv1 = nn.Sequential(\n","            # seq * 5 \n","            nn.Conv2d(1, 15, kernel_size=(3,3), padding=2, bias=False), # (seq+2) * 7\n","            nn.BatchNorm2d(15),\n","            nn.ReLU(inplace=True),\n","\n","            nn.Conv2d(15, 5, kernel_size=(3,3), padding=1, bias=False), # (seq+2) * 7\n","            nn.BatchNorm2d(5),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=(2,2), stride=1), # (seq+1) * 6\n","            \n","        )\n","        \n","        self.conv2 = nn.Sequential(\n","            nn.Conv2d(1, 30, kernel_size=(2,2), padding=0, bias=False), # seq * 5\n","            nn.BatchNorm2d(30),\n","            nn.ReLU(inplace=True),\n","\n","            nn.Conv2d(30, 1, kernel_size=(1,1), padding=0, bias=True), # seq * 5\n","            nn.ReLU(inplace=True)\n","\n","        )\n","\n","    def forward(self, x):\n","        out = self.conv1(x)\n","        out = out.view(out.size()[0], -1)\n","        out = self.linear(out)\n","        # print(out.shape)\n","        with torch.no_grad():\n","            out = out.reshape(self.batch_size, 1, self.seq+1, 6)\n","        out = self.conv2(out)      \n","        return out\n"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-03-16T02:38:21.413224Z","iopub.status.busy":"2023-03-16T02:38:21.412228Z","iopub.status.idle":"2023-03-16T02:38:21.522431Z","shell.execute_reply":"2023-03-16T02:38:21.521131Z","shell.execute_reply.started":"2023-03-16T02:38:21.413168Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\Chen Ding\\AppData\\Local\\Temp\\ipykernel_36112\\1713317878.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(x, dtype=torch.float32)\n"]},{"data":{"text/plain":["[tensor(1.),\n"," tensor(1.),\n"," tensor(1.),\n"," tensor(1.),\n"," tensor(1.),\n"," tensor(1.),\n"," tensor(1.),\n"," tensor(1.),\n"," tensor(1.),\n"," tensor(1.),\n"," tensor(1.),\n"," tensor(1.)]"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["def R_square(A: torch.tensor, B: torch.tensor) -> torch.float32:\n","    assert A.shape == B.shape, \"Predict value not match the Ground Truth\"\n","    # A: predict   B: ground truth\n","    # shape: batch_size * 1 * w * h\n","    A = A.detach()\n","    B = B.detach()\n","    A = A.squeeze()\n","    B = B.squeeze()\n","    flag = len(A.shape)==3\n","    # batch_size * w * h\n","    *_, h = A.shape\n","#     pre_bar = torch.mean(A, dim=[0,1], keepdim=False)\n","    gt_bar = torch.mean(B, dim=[0,1] if flag else 0, keepdim=False)\n","    # print(pre_bar.shape[0])\n","\n","    def sq_sum(x):\n","        # print(x.shape)\n","        x = torch.tensor(x, dtype=torch.float32)\n","        return torch.sum(x * x, dim=[0,1] if flag else 0)\n","    # print(A[:,:,1].shape, pre_bar[1].shape)\n","    SST = [sq_sum(A[:,:,i] if flag else A[:,i] - gt_bar[i]) for i in range(h)]\n","    SSR = [sq_sum(B[:,:,i] if flag else A[:,i] - gt_bar[i]) for i in range(h)]\n","\n","\n","    return [ (SST[i] / SSR[i]) for i in range(h) ]\n","\n","\"\"\"\n","R-squared = SSR / SST = 1 - SSE / SST\n","\"\"\"\n","A = torch.arange(48.*2).reshape(2,1,4,12)   # test\n","R_square(A, A)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-03-16T02:38:21.525025Z","iopub.status.busy":"2023-03-16T02:38:21.523944Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Start Training...\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/312 [00:00<?, ?it/s]C:\\Users\\Chen Ding\\AppData\\Local\\Temp\\ipykernel_2120\\1713317878.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  x = torch.tensor(x, dtype=torch.float32)\n","100%|██████████| 312/312 [00:01<00:00, 244.40it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[ 1|5000] 1.29(s) Train_Loss=54.538317 "]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 312/312 [00:01<00:00, 269.41it/s]\n","100%|██████████| 312/312 [00:01<00:00, 261.09it/s]\n","100%|██████████| 312/312 [00:01<00:00, 273.53it/s]\n","100%|██████████| 312/312 [00:01<00:00, 247.46it/s]\n","100%|██████████| 312/312 [00:01<00:00, 254.46it/s]\n","100%|██████████| 312/312 [00:01<00:00, 254.23it/s]\n","100%|██████████| 312/312 [00:01<00:00, 243.74it/s]\n","100%|██████████| 312/312 [00:01<00:00, 240.00it/s]\n","100%|██████████| 312/312 [00:01<00:00, 250.82it/s]\n","100%|██████████| 312/312 [00:01<00:00, 253.99it/s]\n","100%|██████████| 312/312 [00:01<00:00, 261.45it/s]\n","100%|██████████| 312/312 [00:01<00:00, 235.07it/s]\n","100%|██████████| 312/312 [00:01<00:00, 259.03it/s]\n","100%|██████████| 312/312 [00:01<00:00, 260.63it/s]\n","100%|██████████| 312/312 [00:01<00:00, 256.32it/s]\n","100%|██████████| 312/312 [00:01<00:00, 247.86it/s]\n","100%|██████████| 312/312 [00:01<00:00, 243.11it/s]\n","100%|██████████| 312/312 [00:01<00:00, 240.33it/s]\n","100%|██████████| 312/312 [00:01<00:00, 229.09it/s]\n","100%|██████████| 312/312 [00:01<00:00, 257.95it/s]\n","100%|██████████| 312/312 [00:01<00:00, 266.18it/s]\n","100%|██████████| 312/312 [00:01<00:00, 258.23it/s]\n","100%|██████████| 312/312 [00:01<00:00, 267.32it/s]\n","100%|██████████| 312/312 [00:01<00:00, 271.95it/s]\n","100%|██████████| 312/312 [00:01<00:00, 265.51it/s]\n","100%|██████████| 312/312 [00:01<00:00, 267.41it/s]\n","100%|██████████| 312/312 [00:01<00:00, 270.55it/s]\n","100%|██████████| 312/312 [00:01<00:00, 268.47it/s]\n","100%|██████████| 312/312 [00:01<00:00, 261.29it/s]\n","100%|██████████| 312/312 [00:01<00:00, 266.48it/s]\n","100%|██████████| 312/312 [00:01<00:00, 255.60it/s]\n","100%|██████████| 312/312 [00:01<00:00, 277.03it/s]\n","100%|██████████| 312/312 [00:01<00:00, 270.71it/s]\n","100%|██████████| 312/312 [00:01<00:00, 271.44it/s]\n","100%|██████████| 312/312 [00:01<00:00, 262.05it/s]\n","100%|██████████| 312/312 [00:01<00:00, 264.16it/s]\n","100%|██████████| 312/312 [00:01<00:00, 268.12it/s]\n","100%|██████████| 312/312 [00:01<00:00, 264.58it/s]\n","100%|██████████| 312/312 [00:01<00:00, 269.25it/s]\n","100%|██████████| 312/312 [00:01<00:00, 265.59it/s]\n","100%|██████████| 312/312 [00:01<00:00, 259.95it/s]\n","100%|██████████| 312/312 [00:01<00:00, 275.80it/s]\n","100%|██████████| 312/312 [00:01<00:00, 266.81it/s]\n","100%|██████████| 312/312 [00:01<00:00, 269.44it/s]\n","100%|██████████| 312/312 [00:01<00:00, 267.28it/s]\n","100%|██████████| 312/312 [00:01<00:00, 263.73it/s]\n","100%|██████████| 312/312 [00:01<00:00, 263.73it/s]\n","100%|██████████| 312/312 [00:01<00:00, 273.32it/s]\n","100%|██████████| 312/312 [00:01<00:00, 254.18it/s]\n","100%|██████████| 312/312 [00:01<00:00, 263.97it/s]\n","100%|██████████| 312/312 [00:01<00:00, 265.03it/s]\n","100%|██████████| 312/312 [00:01<00:00, 258.36it/s]\n","100%|██████████| 312/312 [00:01<00:00, 267.44it/s]\n","100%|██████████| 312/312 [00:01<00:00, 262.14it/s]\n","100%|██████████| 312/312 [00:01<00:00, 269.17it/s]\n","100%|██████████| 312/312 [00:01<00:00, 264.41it/s]\n","100%|██████████| 312/312 [00:01<00:00, 256.66it/s]\n","100%|██████████| 312/312 [00:01<00:00, 260.84it/s]\n","100%|██████████| 312/312 [00:01<00:00, 261.23it/s]\n","100%|██████████| 312/312 [00:01<00:00, 264.12it/s]\n","100%|██████████| 312/312 [00:01<00:00, 265.55it/s]\n","100%|██████████| 312/312 [00:01<00:00, 252.10it/s]\n","100%|██████████| 312/312 [00:01<00:00, 256.52it/s]\n","100%|██████████| 312/312 [00:01<00:00, 266.23it/s]\n","100%|██████████| 312/312 [00:01<00:00, 263.70it/s]\n","100%|██████████| 312/312 [00:01<00:00, 256.09it/s]\n","100%|██████████| 312/312 [00:01<00:00, 248.54it/s]\n","100%|██████████| 312/312 [00:01<00:00, 263.90it/s]\n","100%|██████████| 312/312 [00:01<00:00, 258.56it/s]\n","100%|██████████| 312/312 [00:01<00:00, 251.38it/s]\n","100%|██████████| 312/312 [00:01<00:00, 259.44it/s]\n","100%|██████████| 312/312 [00:01<00:00, 265.27it/s]\n","100%|██████████| 312/312 [00:01<00:00, 269.83it/s]\n","100%|██████████| 312/312 [00:01<00:00, 262.40it/s]\n","100%|██████████| 312/312 [00:01<00:00, 259.31it/s]\n","100%|██████████| 312/312 [00:01<00:00, 243.08it/s]\n","100%|██████████| 312/312 [00:01<00:00, 252.62it/s]\n","100%|██████████| 312/312 [00:01<00:00, 257.89it/s]\n","100%|██████████| 312/312 [00:01<00:00, 254.84it/s]\n","100%|██████████| 312/312 [00:01<00:00, 233.49it/s]\n","100%|██████████| 312/312 [00:01<00:00, 243.29it/s]\n","100%|██████████| 312/312 [00:01<00:00, 245.75it/s]\n","100%|██████████| 312/312 [00:01<00:00, 247.51it/s]\n","100%|██████████| 312/312 [00:01<00:00, 248.51it/s]\n","100%|██████████| 312/312 [00:01<00:00, 253.35it/s]\n","100%|██████████| 312/312 [00:01<00:00, 256.47it/s]\n","100%|██████████| 312/312 [00:01<00:00, 204.48it/s]\n","100%|██████████| 312/312 [00:01<00:00, 239.89it/s]\n","100%|██████████| 312/312 [00:01<00:00, 247.20it/s]\n","100%|██████████| 312/312 [00:01<00:00, 238.17it/s]\n","100%|██████████| 312/312 [00:01<00:00, 241.40it/s]\n","100%|██████████| 312/312 [00:01<00:00, 252.25it/s]\n","100%|██████████| 312/312 [00:01<00:00, 234.70it/s]\n","100%|██████████| 312/312 [00:01<00:00, 244.52it/s]\n","100%|██████████| 312/312 [00:01<00:00, 257.12it/s]\n","100%|██████████| 312/312 [00:01<00:00, 253.20it/s]\n","100%|██████████| 312/312 [00:01<00:00, 254.39it/s]\n","100%|██████████| 312/312 [00:01<00:00, 255.32it/s]\n","100%|██████████| 312/312 [00:01<00:00, 254.59it/s]\n","100%|██████████| 312/312 [00:01<00:00, 249.92it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[101|5000] 1.25(s) Train_Loss=54.472571 "]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 312/312 [00:01<00:00, 238.79it/s]\n","100%|██████████| 312/312 [00:01<00:00, 254.13it/s]\n","100%|██████████| 312/312 [00:01<00:00, 236.53it/s]\n","100%|██████████| 312/312 [00:01<00:00, 259.32it/s]\n","100%|██████████| 312/312 [00:01<00:00, 251.10it/s]\n","100%|██████████| 312/312 [00:01<00:00, 255.68it/s]\n","100%|██████████| 312/312 [00:01<00:00, 252.69it/s]\n","100%|██████████| 312/312 [00:01<00:00, 246.90it/s]\n","100%|██████████| 312/312 [00:01<00:00, 244.92it/s]\n","100%|██████████| 312/312 [00:01<00:00, 253.83it/s]\n","100%|██████████| 312/312 [00:01<00:00, 246.77it/s]\n","100%|██████████| 312/312 [00:01<00:00, 235.47it/s]\n","100%|██████████| 312/312 [00:01<00:00, 249.37it/s]\n","100%|██████████| 312/312 [00:01<00:00, 258.27it/s]\n","100%|██████████| 312/312 [00:01<00:00, 256.96it/s]\n","100%|██████████| 312/312 [00:01<00:00, 259.43it/s]\n","100%|██████████| 312/312 [00:01<00:00, 257.43it/s]\n","100%|██████████| 312/312 [00:01<00:00, 258.15it/s]\n","100%|██████████| 312/312 [00:01<00:00, 260.04it/s]\n","100%|██████████| 312/312 [00:01<00:00, 257.61it/s]\n","100%|██████████| 312/312 [00:01<00:00, 250.03it/s]\n","100%|██████████| 312/312 [00:01<00:00, 261.06it/s]\n","100%|██████████| 312/312 [00:01<00:00, 256.79it/s]\n","100%|██████████| 312/312 [00:01<00:00, 259.69it/s]\n","100%|██████████| 312/312 [00:01<00:00, 247.43it/s]\n","100%|██████████| 312/312 [00:01<00:00, 245.40it/s]\n","100%|██████████| 312/312 [00:01<00:00, 255.94it/s]\n","100%|██████████| 312/312 [00:01<00:00, 259.71it/s]\n","100%|██████████| 312/312 [00:01<00:00, 255.55it/s]\n","100%|██████████| 312/312 [00:01<00:00, 262.39it/s]\n","100%|██████████| 312/312 [00:01<00:00, 261.71it/s]\n","100%|██████████| 312/312 [00:01<00:00, 231.23it/s]\n","100%|██████████| 312/312 [00:01<00:00, 255.90it/s]\n","100%|██████████| 312/312 [00:01<00:00, 259.59it/s]\n","100%|██████████| 312/312 [00:01<00:00, 262.14it/s]\n","100%|██████████| 312/312 [00:01<00:00, 259.65it/s]\n","100%|██████████| 312/312 [00:01<00:00, 250.77it/s]\n","100%|██████████| 312/312 [00:01<00:00, 257.58it/s]\n","100%|██████████| 312/312 [00:01<00:00, 257.56it/s]\n","100%|██████████| 312/312 [00:01<00:00, 262.42it/s]\n","100%|██████████| 312/312 [00:01<00:00, 260.68it/s]\n","100%|██████████| 312/312 [00:01<00:00, 260.67it/s]\n","100%|██████████| 312/312 [00:01<00:00, 257.10it/s]\n","100%|██████████| 312/312 [00:01<00:00, 256.42it/s]\n","100%|██████████| 312/312 [00:01<00:00, 232.33it/s]\n","100%|██████████| 312/312 [00:01<00:00, 260.98it/s]\n","100%|██████████| 312/312 [00:01<00:00, 241.06it/s]\n","100%|██████████| 312/312 [00:01<00:00, 263.06it/s]\n","100%|██████████| 312/312 [00:01<00:00, 262.83it/s]\n","100%|██████████| 312/312 [00:01<00:00, 261.91it/s]\n","100%|██████████| 312/312 [00:01<00:00, 260.45it/s]\n","100%|██████████| 312/312 [00:01<00:00, 264.96it/s]\n","100%|██████████| 312/312 [00:01<00:00, 264.55it/s]\n","100%|██████████| 312/312 [00:01<00:00, 252.71it/s]\n","100%|██████████| 312/312 [00:01<00:00, 227.81it/s]\n","100%|██████████| 312/312 [00:01<00:00, 255.73it/s]\n","100%|██████████| 312/312 [00:01<00:00, 259.13it/s]\n","100%|██████████| 312/312 [00:01<00:00, 233.19it/s]\n","100%|██████████| 312/312 [00:01<00:00, 263.56it/s]\n","100%|██████████| 312/312 [00:01<00:00, 265.51it/s]\n","100%|██████████| 312/312 [00:01<00:00, 254.13it/s]\n","100%|██████████| 312/312 [00:01<00:00, 272.18it/s]\n","100%|██████████| 312/312 [00:01<00:00, 248.32it/s]\n","100%|██████████| 312/312 [00:01<00:00, 259.17it/s]\n","100%|██████████| 312/312 [00:01<00:00, 257.04it/s]\n","100%|██████████| 312/312 [00:01<00:00, 257.37it/s]\n","100%|██████████| 312/312 [00:01<00:00, 268.65it/s]\n","100%|██████████| 312/312 [00:01<00:00, 265.96it/s]\n","100%|██████████| 312/312 [00:01<00:00, 264.33it/s]\n","100%|██████████| 312/312 [00:01<00:00, 265.68it/s]\n","100%|██████████| 312/312 [00:01<00:00, 262.35it/s]\n","100%|██████████| 312/312 [00:01<00:00, 270.28it/s]\n","100%|██████████| 312/312 [00:01<00:00, 268.55it/s]\n","100%|██████████| 312/312 [00:01<00:00, 269.04it/s]\n","100%|██████████| 312/312 [00:01<00:00, 268.86it/s]\n","100%|██████████| 312/312 [00:01<00:00, 270.83it/s]\n","100%|██████████| 312/312 [00:01<00:00, 269.61it/s]\n","100%|██████████| 312/312 [00:01<00:00, 237.55it/s]\n","100%|██████████| 312/312 [00:01<00:00, 253.74it/s]\n","100%|██████████| 312/312 [00:01<00:00, 262.63it/s]\n","100%|██████████| 312/312 [00:01<00:00, 263.35it/s]\n","100%|██████████| 312/312 [00:01<00:00, 262.05it/s]\n","100%|██████████| 312/312 [00:01<00:00, 274.32it/s]\n","100%|██████████| 312/312 [00:01<00:00, 275.27it/s]\n","100%|██████████| 312/312 [00:01<00:00, 250.92it/s]\n","100%|██████████| 312/312 [00:01<00:00, 272.16it/s]\n","100%|██████████| 312/312 [00:01<00:00, 273.54it/s]\n","100%|██████████| 312/312 [00:01<00:00, 267.82it/s]\n","100%|██████████| 312/312 [00:01<00:00, 271.77it/s]\n","100%|██████████| 312/312 [00:01<00:00, 268.93it/s]\n","100%|██████████| 312/312 [00:01<00:00, 272.54it/s]\n","100%|██████████| 312/312 [00:01<00:00, 271.80it/s]\n","100%|██████████| 312/312 [00:01<00:00, 261.25it/s]\n","100%|██████████| 312/312 [00:01<00:00, 266.42it/s]\n","100%|██████████| 312/312 [00:01<00:00, 267.03it/s]\n","100%|██████████| 312/312 [00:01<00:00, 268.32it/s]\n","100%|██████████| 312/312 [00:01<00:00, 258.48it/s]\n","100%|██████████| 312/312 [00:01<00:00, 273.57it/s]\n","100%|██████████| 312/312 [00:01<00:00, 271.37it/s]\n","100%|██████████| 312/312 [00:01<00:00, 272.03it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[201|5000] 1.15(s) Train_Loss=54.456183 "]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 312/312 [00:01<00:00, 265.81it/s]\n","100%|██████████| 312/312 [00:01<00:00, 270.35it/s]\n","100%|██████████| 312/312 [00:01<00:00, 272.56it/s]\n","100%|██████████| 312/312 [00:01<00:00, 270.64it/s]\n","100%|██████████| 312/312 [00:01<00:00, 270.37it/s]\n","100%|██████████| 312/312 [00:01<00:00, 275.69it/s]\n","100%|██████████| 312/312 [00:01<00:00, 274.99it/s]\n","100%|██████████| 312/312 [00:01<00:00, 268.06it/s]\n","100%|██████████| 312/312 [00:01<00:00, 246.29it/s]\n","100%|██████████| 312/312 [00:01<00:00, 261.66it/s]\n","100%|██████████| 312/312 [00:01<00:00, 261.29it/s]\n","100%|██████████| 312/312 [00:01<00:00, 253.10it/s]\n","100%|██████████| 312/312 [00:01<00:00, 262.87it/s]\n","100%|██████████| 312/312 [00:01<00:00, 265.65it/s]\n","100%|██████████| 312/312 [00:01<00:00, 262.32it/s]\n","100%|██████████| 312/312 [00:01<00:00, 266.68it/s]\n","100%|██████████| 312/312 [00:01<00:00, 276.33it/s]\n","100%|██████████| 312/312 [00:01<00:00, 279.03it/s]\n","100%|██████████| 312/312 [00:01<00:00, 250.64it/s]\n","100%|██████████| 312/312 [00:01<00:00, 274.22it/s]\n","100%|██████████| 312/312 [00:01<00:00, 271.41it/s]\n","100%|██████████| 312/312 [00:01<00:00, 270.74it/s]\n","100%|██████████| 312/312 [00:01<00:00, 274.37it/s]\n","100%|██████████| 312/312 [00:01<00:00, 270.01it/s]\n","100%|██████████| 312/312 [00:01<00:00, 272.02it/s]\n","100%|██████████| 312/312 [00:01<00:00, 272.09it/s]\n","100%|██████████| 312/312 [00:01<00:00, 274.38it/s]\n","100%|██████████| 312/312 [00:01<00:00, 273.29it/s]\n","100%|██████████| 312/312 [00:01<00:00, 261.52it/s]\n","100%|██████████| 312/312 [00:01<00:00, 261.21it/s]\n","100%|██████████| 312/312 [00:01<00:00, 245.70it/s]\n","100%|██████████| 312/312 [00:01<00:00, 244.33it/s]\n","100%|██████████| 312/312 [00:01<00:00, 242.25it/s]\n","100%|██████████| 312/312 [00:01<00:00, 252.26it/s]\n","100%|██████████| 312/312 [00:01<00:00, 239.30it/s]\n","100%|██████████| 312/312 [00:01<00:00, 256.99it/s]\n","100%|██████████| 312/312 [00:01<00:00, 257.85it/s]\n","100%|██████████| 312/312 [00:01<00:00, 247.30it/s]\n","100%|██████████| 312/312 [00:01<00:00, 256.30it/s]\n","100%|██████████| 312/312 [00:01<00:00, 261.84it/s]\n"," 15%|█▌        | 48/312 [00:00<00:01, 248.77it/s]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[17], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     24\u001b[0m \u001b[39m# if use[0].shape[0]==2:\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[39m#     print(use[0])\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m train_pred \u001b[39m=\u001b[39m net(use[\u001b[39m0\u001b[39;49m])    \u001b[39m# use[0].cuda()\u001b[39;00m\n\u001b[0;32m     28\u001b[0m batch_loss \u001b[39m=\u001b[39m Loss(train_pred, use[\u001b[39m1\u001b[39m])   \u001b[39m# use[1].cuda()\u001b[39;00m\n\u001b[0;32m     29\u001b[0m batch_loss\u001b[39m.\u001b[39mbackward()\n","File \u001b[1;32md:\\Download\\Ana\\envs\\ldm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","Cell \u001b[1;32mIn[14], line 49\u001b[0m, in \u001b[0;36mTry.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m     48\u001b[0m     out \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mreshape(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size, \u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseq\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m6\u001b[39m)\n\u001b[1;32m---> 49\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv2(out)      \n\u001b[0;32m     50\u001b[0m \u001b[39mreturn\u001b[39;00m out\n","File \u001b[1;32md:\\Download\\Ana\\envs\\ldm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32md:\\Download\\Ana\\envs\\ldm\\lib\\site-packages\\torch\\nn\\modules\\container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n","File \u001b[1;32md:\\Download\\Ana\\envs\\ldm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32md:\\Download\\Ana\\envs\\ldm\\lib\\site-packages\\torch\\nn\\modules\\conv.py:457\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 457\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n","File \u001b[1;32md:\\Download\\Ana\\envs\\ldm\\lib\\site-packages\\torch\\nn\\modules\\conv.py:453\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    450\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    451\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    452\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 453\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    454\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["x_plt, train_loss_plt = [], []\n","global lr, num_epoch\n","lr, num_epoch = 0.0001, 5000\n","\n","\n","net = Try(batch_size=batch_size, seq=15)\n","Loss = torch.nn.MSELoss()\n","optimizer = torch.optim.Adam(net.parameters(), lr)\n","scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n","\n","def Iter(num_epoch):\n","    cnt = 0\n","    while cnt < num_epoch:\n","        yield cnt\n","        cnt += 1\n","\n","print(\"Start Training...\")\n","for epoch in range(num_epoch):\n","    epoch_start_time = time.time()\n","    train_loss = 0.0\n","    net.train()\n","    for i, use in enumerate(tqdm(Data_Iter)):\n","        optimizer.zero_grad()\n","        # if use[0].shape[0]==2:\n","        #     print(use[0])\n","        train_pred = net(use[0])    # use[0].cuda()\n","\n","        batch_loss = Loss(train_pred, use[1])   # use[1].cuda()\n","        batch_loss.backward()\n","        optimizer.step()\n","        R2 = R_square(train_pred.cpu(), use[1].cpu())\n","\n","        train_loss += batch_loss.item()\n","\n","    train_loss = train_loss / train_size\n","    x_plt.append(epoch+1)\n","    train_loss_plt.append(train_loss)\n","    if epoch%100 == 0:\n","        print(\"[%2d|%2d] %.2f(s) Train_Loss=%.6f \"%\\\n","            (epoch+1,num_epoch,time.time()-epoch_start_time,train_loss),end='')\n","        epoch_start_time = time.time()\n","    scheduler.step()  \n","\n","plt.figure(1)\n","plt.plot(x_plt,train_loss_plt,'rs-',label='all_train_loss')\n","plt.show()\n","torch.save(net.state_dict(), './data/model_cnn.pt')\n","print(\"Parameters Saved.\")"]},{"cell_type":"code","execution_count":18,"metadata":{"trusted":true},"outputs":[{"ename":"RuntimeError","evalue":"Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[1;32mIn[18], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m Test_Iter \u001b[39m=\u001b[39m DataLoader(dataset\u001b[39m=\u001b[39mtest_set, batch_size\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, drop_last\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      9\u001b[0m model \u001b[39m=\u001b[39m Try(batch_size\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, seq\u001b[39m=\u001b[39m\u001b[39m15\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m model\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39m./data/model_cnn.pt\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[0;32m     11\u001b[0m \u001b[39m# model = model.cuda()\u001b[39;00m\n\u001b[0;32m     12\u001b[0m model\u001b[39m.\u001b[39meval()\n","File \u001b[1;32md:\\Download\\Ana\\envs\\ldm\\lib\\site-packages\\torch\\serialization.py:712\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    710\u001b[0m             opened_file\u001b[39m.\u001b[39mseek(orig_position)\n\u001b[0;32m    711\u001b[0m             \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mload(opened_file)\n\u001b[1;32m--> 712\u001b[0m         \u001b[39mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpickle_load_args)\n\u001b[0;32m    713\u001b[0m \u001b[39mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n","File \u001b[1;32md:\\Download\\Ana\\envs\\ldm\\lib\\site-packages\\torch\\serialization.py:1049\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1047\u001b[0m unpickler \u001b[39m=\u001b[39m UnpicklerWrapper(data_file, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1048\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[1;32m-> 1049\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[0;32m   1051\u001b[0m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[0;32m   1053\u001b[0m \u001b[39mreturn\u001b[39;00m result\n","File \u001b[1;32md:\\Download\\Ana\\envs\\ldm\\lib\\site-packages\\torch\\serialization.py:1019\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[1;34m(saved_id)\u001b[0m\n\u001b[0;32m   1017\u001b[0m \u001b[39mif\u001b[39;00m key \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m loaded_storages:\n\u001b[0;32m   1018\u001b[0m     nbytes \u001b[39m=\u001b[39m numel \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_element_size(dtype)\n\u001b[1;32m-> 1019\u001b[0m     load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))\n\u001b[0;32m   1021\u001b[0m \u001b[39mreturn\u001b[39;00m loaded_storages[key]\n","File \u001b[1;32md:\\Download\\Ana\\envs\\ldm\\lib\\site-packages\\torch\\serialization.py:1001\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[1;34m(dtype, numel, key, location)\u001b[0m\n\u001b[0;32m    997\u001b[0m storage \u001b[39m=\u001b[39m zip_file\u001b[39m.\u001b[39mget_storage_from_record(name, numel, torch\u001b[39m.\u001b[39m_UntypedStorage)\u001b[39m.\u001b[39mstorage()\u001b[39m.\u001b[39m_untyped()\n\u001b[0;32m    998\u001b[0m \u001b[39m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[0;32m    999\u001b[0m \u001b[39m# stop wrapping with _TypedStorage\u001b[39;00m\n\u001b[0;32m   1000\u001b[0m loaded_storages[key] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstorage\u001b[39m.\u001b[39m_TypedStorage(\n\u001b[1;32m-> 1001\u001b[0m     wrap_storage\u001b[39m=\u001b[39mrestore_location(storage, location),\n\u001b[0;32m   1002\u001b[0m     dtype\u001b[39m=\u001b[39mdtype)\n","File \u001b[1;32md:\\Download\\Ana\\envs\\ldm\\lib\\site-packages\\torch\\serialization.py:175\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[1;34m(storage, location)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_restore_location\u001b[39m(storage, location):\n\u001b[0;32m    174\u001b[0m     \u001b[39mfor\u001b[39;00m _, _, fn \u001b[39min\u001b[39;00m _package_registry:\n\u001b[1;32m--> 175\u001b[0m         result \u001b[39m=\u001b[39m fn(storage, location)\n\u001b[0;32m    176\u001b[0m         \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    177\u001b[0m             \u001b[39mreturn\u001b[39;00m result\n","File \u001b[1;32md:\\Download\\Ana\\envs\\ldm\\lib\\site-packages\\torch\\serialization.py:152\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[1;34m(obj, location)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_cuda_deserialize\u001b[39m(obj, location):\n\u001b[0;32m    151\u001b[0m     \u001b[39mif\u001b[39;00m location\u001b[39m.\u001b[39mstartswith(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m--> 152\u001b[0m         device \u001b[39m=\u001b[39m validate_cuda_device(location)\n\u001b[0;32m    153\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(obj, \u001b[39m\"\u001b[39m\u001b[39m_torch_load_uninitialized\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    154\u001b[0m             \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice(device):\n","File \u001b[1;32md:\\Download\\Ana\\envs\\ldm\\lib\\site-packages\\torch\\serialization.py:136\u001b[0m, in \u001b[0;36mvalidate_cuda_device\u001b[1;34m(location)\u001b[0m\n\u001b[0;32m    133\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_get_device_index(location, \u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    135\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available():\n\u001b[1;32m--> 136\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mAttempting to deserialize object on a CUDA \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    137\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mdevice but torch.cuda.is_available() is False. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    138\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mIf you are running on a CPU-only machine, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    139\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mplease use torch.load with map_location=torch.device(\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39mcpu\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    140\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mto map your storages to the CPU.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    141\u001b[0m device_count \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice_count()\n\u001b[0;32m    142\u001b[0m \u001b[39mif\u001b[39;00m device \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m device_count:\n","\u001b[1;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."]}],"source":["import math\n","import numpy as np\n","from scipy import stats\n","def rsquared(x, y): \n","    _, _, r_value, _, _ = stats.linregress(x.detach().numpy(), y.detach().numpy()) \n","    return r_value**2\n","\n","Test_Iter = DataLoader(dataset=test_set, batch_size=1, shuffle=False, drop_last=True)\n","model = Try(batch_size=1, seq=15)\n","model.load_state_dict(torch.load('./data/model_cnn.pt'))\n","model = model.cuda()\n","model.eval()\n","with torch.no_grad():\n","    for i, use in enumerate(Test_Iter):\n","        pred = model(use[0].cuda())\n","#         print(pred.shape, use[1].shape)\n","        R = rsquared(pred, use[1].cuda())\n","        print(i, R)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":4}

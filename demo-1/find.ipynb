{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils import data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "def rsquared(x, y): \n",
    "    _, _, r_value, _, _ = stats.linregress(x.detach().cpu().numpy(), y.detach().cpu().numpy()) \n",
    "    return r_value**2\n",
    "\n",
    "\n",
    "def normalization(x: list):\n",
    "    M, m = np.max(x), np.min(x)\n",
    "    for i in range(len(x)):\n",
    "        x[i] = (x[i] - (M + m) / 2) / ((M - m) / 2)\n",
    "    # x in [-1, 1]\n",
    "    return M, m, x\n",
    "def cal(x: list):\n",
    "    mu, sigma = 0, 0\n",
    "    for i in range(len(x)):\n",
    "        mu += x[i]\n",
    "    mu /= len(x)*1.0\n",
    "    for i in range(len(x)):\n",
    "        sigma += (x[i]-mu) ** 2\n",
    "    sigma /= len(x)*1.0\n",
    "    from math import sqrt\n",
    "    for i in range(len(x)):\n",
    "        x[i] = (x[i]-mu) / sqrt(sigma)\n",
    "    return mu, sigma, x\n",
    "\n",
    "def ArrNorm(x: np.ndarray, state='max-min'):\n",
    "    assert isinstance(x, np.ndarray), \"We need a list\"\n",
    "    \n",
    "    if state == 'max-min':\n",
    "        M_list, m_list, res = [], [], []\n",
    "        for i in range(x.shape[0]):\n",
    "            u = x[i].tolist()\n",
    "            M, m, t = normalization(u)\n",
    "            res.append(t)\n",
    "            M_list.append(M)\n",
    "            m_list.append(m)\n",
    "        return M_list, m_list, np.array(res)\n",
    "    elif state == 'standard':\n",
    "        mu_list, sigma_list, res = [], [], []\n",
    "        for i in range(x.shape[0]):\n",
    "            u = x[i].tolist()\n",
    "            m, s, t = cal(u)\n",
    "            res.append(t)\n",
    "            mu_list.append(m)\n",
    "            sigma_list.append(s)\n",
    "        return mu_list, sigma_list, res\n",
    "\n",
    "\n",
    "def df2arr(x) -> np.ndarray:\n",
    "    return np.array(x, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NET(nn.Module):\n",
    "    def __init__(self, seq, batch_size, ablation_scale=1.0):\n",
    "        super(NET, self).__init__()\n",
    "        self.seq = seq\n",
    "        self.batch_size = batch_size\n",
    "        self.scale = ablation_scale\n",
    "        self.input_size = self.hidden_size = seq\n",
    "        \n",
    "        self.is_directional = False\n",
    "        self.num_direction = 2 if self.is_directional else 1\n",
    "        self.num_layers = 1\n",
    "        \n",
    "        self.lstm = nn.LSTM(self.input_size, self.hidden_size, self.num_layers, dropout=0.5, bidirectional=self.is_directional, batch_first=False)\n",
    "        \n",
    "        self.decouple = nn.Sequential(\n",
    "            nn.Linear(self.seq*5,self.seq*5),  # linear decouple\n",
    "            nn.Dropout(0.5),\n",
    "#             nn.Sigmoid(),   # use sigmoid to enhance unlinear regression may cause data shifting\n",
    "            nn.Linear(self.seq*5, self.seq*5),     # unlinear pool\n",
    "        )\n",
    "        self.conv = nn.Sequential(\n",
    "            # seq * 5 \n",
    "            nn.Conv2d(1, 2, kernel_size=(3,3), padding=1, bias=False), # seq * 5\n",
    "            nn.BatchNorm2d(2),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(2, 2, kernel_size=(1,1), padding=0, bias=False),  # seq * 5\n",
    "            nn.BatchNorm2d(2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(2, 1, kernel_size=(3,3), padding=1, bias=True), # seq * 5\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "    \n",
    "    def reset(self, scale=1.0):\n",
    "        self.ablatiion_scale = scale\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.squeeze()\n",
    "        \n",
    "        h_0 = torch.randn(self.num_direction*self.num_layers, self.seq, self.hidden_size).cuda()\n",
    "        c_0 = torch.randn(self.num_direction*self.num_layers, self.seq, self.hidden_size).cuda()\n",
    "        pred, *_ = self.lstm(x, (h_0, c_0))\n",
    "        pred = pred.flatten(1)\n",
    "        out1 = self.decouple(pred)\n",
    "        with torch.no_grad():\n",
    "            out1 = out1.reshape(-1, 1, self.seq, 5)\n",
    "        out2 = self.conv(out1)\n",
    "        assert out1.shape==out2.shape, \"Shape Unequal Error.\"\n",
    "        return self.scale * out1 + out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel = pd.read_excel('/kaggle/input/a32-data/A32.xlsx', header=None)\n",
    "excel.shape\n",
    "\n",
    "sp = [1486, 2972, 4458]\n",
    "station_1 = excel.iloc[1:sp[0]+1,1:6]\n",
    "station_2 = excel.iloc[sp[0]+1:sp[1]+1,1:6]\n",
    "standard = excel.iloc[sp[1]+1:sp[2]+1,1:6]\n",
    "\n",
    "station_1 = df2arr(station_1)\n",
    "station_2 = df2arr(station_2)\n",
    "standard = df2arr(standard)\n",
    "station_1.shape, station_2.shape, standard.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_minus_sd = station_1 - standard\n",
    "s2_minus_sd = station_2 - standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_M, s1_m, s1 = ArrNorm(station_1, 'standard')\n",
    "s2_M, s2_m, s2 = ArrNorm(station_2, 'standard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetDataset(input_arr: list, output_arr: list, seq: int):\n",
    "    assert(len(input_arr)==len(output_arr)), \"Different size of input and output!\"\n",
    "    Input = []\n",
    "    Output = []\n",
    "    for i in range(len(input_arr)-seq):\n",
    "        Input.append(input_arr[i:i+seq][:])\n",
    "        Output.append(output_arr[i:i+seq][:])\n",
    "    return torch.tensor(Input, dtype=torch.float32), torch.tensor(Output, dtype=torch.float32)\n",
    "\n",
    "        \n",
    "def load_array(data_arrays, batch_size, is_train=True):\n",
    "    # data-iter\n",
    "    dataset = data.TensorDataset(*data_arrays)\n",
    "    return data.DataLoader(dataset, batch_size, shuffle=is_train)\n",
    "\n",
    "\n",
    "s1_minus_sd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Input_Data_1, Output_Data_1 = GetDataset(s1, s1_minus_sd, sequence) \n",
    "Input_Data_2, Output_Data_2 = GetDataset(s2, s2_minus_sd, sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = 5\n",
    "batch_size = 32     # or 16\n",
    "base_scale = 6.0\n",
    "\n",
    "pth_path = ''\n",
    "\n",
    "Test_Iter_1 = load_array((Input_Data_1, Output_Data_1), batch_size=Input_Data_1.shape[0], shuffle=True, drop_last=True)\n",
    "Test_Iter_2 = load_array((Input_Data_2, Output_Data_2), batch_size=Input_Data_2.shape[0], shuffle=True, drop_last=True)\n",
    "\n",
    "model = NET(batch_size=batch_size, seq=sequence)\n",
    "model.load_state_dict(torch.load(pth_path))\n",
    "model = model.cuda()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "R1, R2 = [], []\n",
    "scale1, scale2 = [], []\n",
    "r_1, r_2 = .0, .0\n",
    "scale_1, scale_2 = 0, 0\n",
    "\n",
    "for idx in range(5):\n",
    "    for scale in np.arange(base_scale-5., base_scale+5., .5):\n",
    "        model.reset(scale=scale)\n",
    "        with torch.no_grad():\n",
    "            for i, use in enumerate(Test_Iter_1):\n",
    "                pred = model(use[0].cuda())\n",
    "                pred = pred.reshape(-1, 5)\n",
    "                use[1] = use[1].reshape(-1, 5)\n",
    "                assert pred.shape == use[1].shape, 'unequal shape error'\n",
    "                r1 = rsquared(pred[idx], use[1][idx])\n",
    "            for i, use in enumerate(Test_Iter_2):\n",
    "                    pred = model(use[0].cuda())\n",
    "                    pred = pred.reshape(-1, 5)\n",
    "                    use[1] = use[1].reshape(-1, 5)\n",
    "                    assert pred.shape == use[1].shape, 'unequal shape error'\n",
    "                    r2 = rsquared(pred[idx], use[1][idx])\n",
    "            if r1 > r_1:\n",
    "                r_1 = r1\n",
    "                scale_1 = scale\n",
    "            if r2 > r_2:\n",
    "                r_2 = r2\n",
    "                scale_2 = scale\n",
    "    R1.append(r_1)\n",
    "    R2.append(r_2)\n",
    "    scale_1.append(scale_1)\n",
    "    scale_2.append(scale_2)\n",
    "\n",
    "assert len(R1)==5, 'index error'\n",
    "\n",
    "print(R1)\n",
    "print(R2)\n",
    "print(scale_1)\n",
    "print(scale_2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

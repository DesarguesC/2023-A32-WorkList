{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils import data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "def normalization(x: list):\n",
    "    M, m = np.max(x), np.min(x)\n",
    "    for i in range(len(x)):\n",
    "        x[i] = (x[i] - (M + m) / 2) / ((M - m) / 2)\n",
    "    # x in [-1, 1]\n",
    "    return M, m, x\n",
    "\n",
    "def ArrNorm(x: np.ndarray):\n",
    "    assert isinstance(x, np.ndarray), \"We need a list\"\n",
    "    M_list, m_list, res = [], [], []\n",
    "    for i in range(x.shape[0]):\n",
    "        u = x[i].tolist()\n",
    "        M, m, t = normalization(u)\n",
    "        res.append(t)\n",
    "        M_list.append(M)\n",
    "        m_list.append(m)\n",
    "    return M_list, m_list, np.array(res)\n",
    "\n",
    "\n",
    "def df2arr(x) -> np.ndarray:\n",
    "    return np.array(x, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel = pd.read_excel('./data/A32.xlsx', header=None)\n",
    "excel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = [1486, 2972, 4458]\n",
    "station_1 = excel.iloc[1:sp[0]+1,1:6]\n",
    "station_2 = excel.iloc[sp[0]+1:sp[1]+1,1:6]\n",
    "standard = excel.iloc[sp[1]+1:sp[2]+1,1:6]\n",
    "standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_1 = df2arr(station_1)\n",
    "station_2 = df2arr(station_2)\n",
    "standard = df2arr(standard)\n",
    "station_1.shape, station_2.shape, standard.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_minus_sd = station_1 - standard\n",
    "s2_minus_sd = station_2 - standard\n",
    "s1_div_sd = station_1 / standard\n",
    "s2_dic_sd = station_2 / standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_minus_sd.shape, s2_minus_sd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_minus_sd_M, s1_minus_sd_m, s1_minus_sd = ArrNorm(s1_minus_sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetDataset(input_arr: list, output_arr: list, seq: int):\n",
    "    assert(len(input_arr)==len(output_arr)), \"Different size of input and output!\"\n",
    "    Input = []\n",
    "    Output = []\n",
    "    for i in range(input_arr.shape[0]-seq):\n",
    "        Input.append(input_arr[i:i+seq][:])\n",
    "        Output.append(output_arr[i:i+seq][:])\n",
    "    return torch.tensor(Input, dtype=torch.float32), torch.tensor(Output, dtype=torch.float32)\n",
    "\n",
    "        \n",
    "def load_array(data_arrays, batch_size, is_train=True):\n",
    "    # data-iter\n",
    "    dataset = data.TensorDataset(*data_arrays)\n",
    "    return data.DataLoader(dataset, batch_size, shuffle=is_train)\n",
    "\n",
    "s1_minus_sd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Input_Data_1, Output_Data_1 = GetDataset(s1_minus_sd, standard, 15)\n",
    "Input_Data_1.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one train demo\n",
    "\n",
    "class Try(nn.Module):\n",
    "    def __init__(self, seq, batch_size, scale=0):\n",
    "        super(Try, self).__init__()\n",
    "        self.scale = scale\n",
    "        self.seq = seq\n",
    "        self.into = batch_size\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear((self.seq+1)*6, (self.seq+1)*12),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Linear((self.seq+1)*12, 200),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Linear(200, (self.seq+1)*3),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear((self.seq+1)*3, 200, (self.seq+1)*6),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv = nn.Sequential(\n",
    "            # seq * 5 \n",
    "            nn.Conv2d(self.into, 60, kernel_size=(3,3), padding=2, bias=False), # (seq+2) * 7\n",
    "            nn.BatchNorm2d(60),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(60, 120, kernel_size=(3,3), padding=1, bias=False), # (seq+2) * 7\n",
    "            nn.BatchNorm2d(120),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=(2,2)), # (seq+1) * 6\n",
    "\n",
    "            nn.Conv2d(120, 60, kernel_size=(1,1), padding=0, bias=True), # (seq+1) * 6\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = out.view(out.size()[0], -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "global cr, seed\n",
    "cr = 0.85\n",
    "seed = 2\n",
    "\n",
    "data_tot = torch.utils.data.TensorDataset(Input_Data_1, Output_Data_1)\n",
    "train_size = int(Input_Data_1.shape[0] * cr)\n",
    "test_size = Input_Data_1.shape[0] - train_size\n",
    "train_set , test_set = random_split(data_tot,[train_size,test_size],\n",
    "                                   torch.Generator().manual_seed(0))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# DataIter = load_array((Input_Data_1, Output_Data_1), batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def R_square(A: torch.tensor, B: torch.tensor) -> torch.float32:\n",
    "    assert A.shape == B.shape, \"Predict value not match the Ground Truth\"\n",
    "    # A: predict   B: ground truth\n",
    "    A = A.detach()\n",
    "    B = B.detach()\n",
    "    _, b = A.shape\n",
    "    pre_bar = torch.mean(A, dim=0, keepdim=False)\n",
    "    gt_bar = torch.mean(B, dim=0, keepdim=False)\n",
    "    assert pre_bar.shape[0] == A.shape[1], \"Error\"\n",
    "    assert gt_bar.shape[0] == B.shape[1], \"Error\"\n",
    "    def sq_sum(x):\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        return torch.sum(x * x, dim=0)\n",
    "    # print(A[:, 3])\n",
    "    # print([A[:, i] - pre_bar[i] for i in range(b)])\n",
    "    SST = [sq_sum(A[:, i] - pre_bar[i]) for i in range(b)]\n",
    "    SSR = [sq_sum(B[:, i] - gt_bar[i]) for i in range(b)]\n",
    "\n",
    "\n",
    "    return [ (SST[i] / SSR[i]) for i in range(b) ]\n",
    "\n",
    "\"\"\"\n",
    "R-squared = SSR / SST = 1 - SSE / SST\n",
    "\"\"\"\n",
    "A = torch.arange(12.).reshape(3,4)   # test\n",
    "R_square(A, A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_plt, train_loss_plt = [], []\n",
    "global lr, num_epoch, batch_size\n",
    "lr, num_epoch, batch_size = 0.001, 1000, 8\n",
    "Data_Iter = data.DataLoader(train_set, batch_size, shuffle=True)\n",
    "\n",
    "net = Try(batch_size=batch_size, seq=8)\n",
    "Loss = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr)\n",
    "scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "loss_list = []\n",
    "\n",
    "\n",
    "print(\"Start Training...\")\n",
    "for epoch in range(num_epoch):\n",
    "    epoch_start_time = time.time()\n",
    "    train_loss = 0.0\n",
    "    R2 = .0\n",
    "    net.train()\n",
    "    for i, use in enumerate((Data_Iter)):\n",
    "        optimizer.zero_grad()\n",
    "        print(use[0].shape)\n",
    "        train_pred = net(use[0])    # use[0].cuda()\n",
    "\n",
    "        batch_loss = Loss(train_pred, use[1])   # use[1].cuda()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        R2 = R_square(train_pred, use[1])\n",
    "\n",
    "        train_loss += batch_loss.item()\n",
    "\n",
    "    train_loss = train_loss / train_size\n",
    "    x_plt.append(epoch+2)\n",
    "    train_loss_plt.append(train_loss)\n",
    "    print(\"[%2d|%2d] %.2f(s) R-squared=%.6f Train_Loss=%.6f (%%)\"%\\\n",
    "            (epoch+1,num_epoch,time.time()-epoch_start_time,R2,train_loss))\n",
    "    scheduler.step()\n",
    "\n",
    "plt.figure(1)\n",
    "# plt.plot(x_plt,train_acc_plt,'ob',label='all_train_acc')\n",
    "# plt.figure(2)\n",
    "plt.plot(x_plt,train_loss_plt,'rs-',label='all_train_loss')\n",
    "plt.show()\n",
    "\n",
    "torch.save(net.state_dict(),'./model_cnn'+str(num)+'.pth')\n",
    "print(\"Parameters Saved.\")  \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ldm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

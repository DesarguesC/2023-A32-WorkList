{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils import data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "def normalization(x: list):\n",
    "    M, m = np.max(x), np.min(x)\n",
    "    for i in range(len(x)):\n",
    "        x[i] = (x[i] - (M + m) / 2) / ((M - m) / 2)\n",
    "    # x in [-1, 1]\n",
    "    return M, m, x\n",
    "\n",
    "def ArrNorm(x: np.ndarray):\n",
    "    assert isinstance(x, np.ndarray), \"We need a list\"\n",
    "    M_list, m_list, res = [], [], []\n",
    "    for i in range(x.shape[0]):\n",
    "        u = x[i].tolist()\n",
    "        M, m, t = normalization(u)\n",
    "        res.append(t)\n",
    "        M_list.append(M)\n",
    "        m_list.append(m)\n",
    "    return M_list, m_list, np.array(res)\n",
    "\n",
    "\n",
    "def df2arr(x) -> np.ndarray:\n",
    "    return np.array(x, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4459, 18)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "excel = pd.read_excel('./data/A32.xlsx', header=None)\n",
    "excel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2973</th>\n",
       "      <td>16</td>\n",
       "      <td>33</td>\n",
       "      <td>15</td>\n",
       "      <td>1.5</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2974</th>\n",
       "      <td>16</td>\n",
       "      <td>33</td>\n",
       "      <td>15</td>\n",
       "      <td>1.7</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2975</th>\n",
       "      <td>16</td>\n",
       "      <td>38</td>\n",
       "      <td>16</td>\n",
       "      <td>1.8</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2976</th>\n",
       "      <td>18</td>\n",
       "      <td>41</td>\n",
       "      <td>21</td>\n",
       "      <td>1.9</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2977</th>\n",
       "      <td>17</td>\n",
       "      <td>41</td>\n",
       "      <td>23</td>\n",
       "      <td>1.7</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4454</th>\n",
       "      <td>16</td>\n",
       "      <td>23</td>\n",
       "      <td>49</td>\n",
       "      <td>21</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4455</th>\n",
       "      <td>18</td>\n",
       "      <td>26</td>\n",
       "      <td>55</td>\n",
       "      <td>20.7</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4456</th>\n",
       "      <td>19</td>\n",
       "      <td>28</td>\n",
       "      <td>52</td>\n",
       "      <td>20.8</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4457</th>\n",
       "      <td>22</td>\n",
       "      <td>31</td>\n",
       "      <td>41</td>\n",
       "      <td>21</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4458</th>\n",
       "      <td>18</td>\n",
       "      <td>26</td>\n",
       "      <td>38</td>\n",
       "      <td>21.3</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1486 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       1   2   3     4   5\n",
       "2973  16  33  15   1.5  74\n",
       "2974  16  33  15   1.7  74\n",
       "2975  16  38  16   1.8  76\n",
       "2976  18  41  21   1.9  79\n",
       "2977  17  41  23   1.7  78\n",
       "...   ..  ..  ..   ...  ..\n",
       "4454  16  23  49    21  99\n",
       "4455  18  26  55  20.7  99\n",
       "4456  19  28  52  20.8  99\n",
       "4457  22  31  41    21  98\n",
       "4458  18  26  38  21.3  97\n",
       "\n",
       "[1486 rows x 5 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = [1486, 2972, 4458]\n",
    "station_1 = excel.iloc[1:sp[0]+1,1:6]\n",
    "station_2 = excel.iloc[sp[0]+1:sp[1]+1,1:6]\n",
    "standard = excel.iloc[sp[1]+1:sp[2]+1,1:6]\n",
    "standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1486, 5), (1486, 5), (1486, 5))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "station_1 = df2arr(station_1)\n",
    "station_2 = df2arr(station_2)\n",
    "standard = df2arr(standard)\n",
    "station_1.shape, station_2.shape, standard.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chen Ding\\AppData\\Local\\Temp\\ipykernel_22380\\711113420.py:3: RuntimeWarning: divide by zero encountered in divide\n",
      "  s1_div_sd = station_1 / standard\n",
      "C:\\Users\\Chen Ding\\AppData\\Local\\Temp\\ipykernel_22380\\711113420.py:4: RuntimeWarning: divide by zero encountered in divide\n",
      "  s2_dic_sd = station_2 / standard\n"
     ]
    }
   ],
   "source": [
    "s1_minus_sd = station_1 - standard\n",
    "s2_minus_sd = station_2 - standard\n",
    "s1_div_sd = station_1 / standard\n",
    "s2_dic_sd = station_2 / standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1486, 5), (1486, 5))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1_minus_sd.shape, s2_minus_sd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_minus_sd_M, s1_minus_sd_m, s1_minus_sd = ArrNorm(s1_minus_sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1486, 5)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def GetDataset(input_arr: list, output_arr: list, seq: int):\n",
    "    assert(len(input_arr)==len(output_arr)), \"Different size of input and output!\"\n",
    "    Input = []\n",
    "    Output = []\n",
    "    for i in range(input_arr.shape[0]-seq):\n",
    "        Input.append(input_arr[i:i+seq][:])\n",
    "        Output.append(output_arr[i:i+seq][:])\n",
    "    return torch.tensor(Input, dtype=torch.float32), torch.tensor(Output, dtype=torch.float32)\n",
    "\n",
    "        \n",
    "def load_array(data_arrays, batch_size, is_train=True):\n",
    "    # data-iter\n",
    "    dataset = data.TensorDataset(*data_arrays)\n",
    "    return data.DataLoader(dataset, batch_size, shuffle=is_train)\n",
    "\n",
    "s1_minus_sd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1471, 15, 5])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Input_Data_1, Output_Data_1 = GetDataset(s1_minus_sd, standard, 15)\n",
    "Input_Data_1.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1471, 1, 15, 5]), torch.Size([1471, 1, 15, 5]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Input_Data_1 = Input_Data_1.unsqueeze(1)\n",
    "Output_Data_1 = Output_Data_1.unsqueeze(1)\n",
    "Input_Data_1.shape, Output_Data_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one train demo\n",
    "\n",
    "class Try(nn.Module):\n",
    "    def __init__(self, seq, batch_size, scale=0):\n",
    "        super(Try, self).__init__()\n",
    "        self.scale = scale\n",
    "        self.seq = seq\n",
    "        self.batch_size = batch_size\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear((self.seq+1)*6*60, (self.seq+1)*60),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Sigmoid(),\n",
    "\n",
    "            nn.Linear((self.seq+1)*60, (self.seq+1)*6),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Linear((self.seq+1)*6, (self.seq+1)*6),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv1 = nn.Sequential(\n",
    "            # seq * 5 \n",
    "            nn.Conv2d(1, 30, kernel_size=(3,3), padding=2, bias=False), # (seq+2) * 7\n",
    "            nn.BatchNorm2d(30),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(30, 60, kernel_size=(3,3), padding=1, bias=False), # (seq+2) * 7\n",
    "            nn.BatchNorm2d(60),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=(2,2), stride=1), # (seq+1) * 6\n",
    "            \n",
    "        )\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(1, 30, kernel_size=(2,2), padding=0, bias=False), # seq * 5\n",
    "            nn.BatchNorm2d(30),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(30, 1, kernel_size=(1,1), padding=0, bias=True), # seq * 5\n",
    "            nn.ReLU(inplace=True)\n",
    "\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = out.view(out.size()[0], -1)\n",
    "        out = self.linear(out)\n",
    "        # print(out.shape)\n",
    "        with torch.no_grad():\n",
    "            out = out.reshape(self.batch_size, 1, self.seq+1, 6)\n",
    "        out = self.conv2(out)      \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "global cr, seed\n",
    "cr = 0.85\n",
    "seed = 2\n",
    "\n",
    "data_tot = torch.utils.data.TensorDataset(Input_Data_1, Output_Data_1)\n",
    "train_size = int(Input_Data_1.shape[0] * cr)\n",
    "test_size = Input_Data_1.shape[0] - train_size\n",
    "train_set , test_set = random_split(data_tot,[train_size,test_size],\n",
    "                                   torch.Generator().manual_seed(0))\n",
    "# DataIter = load_array((Input_Data_1, Output_Data_1), batch_size=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chen Ding\\AppData\\Local\\Temp\\ipykernel_22380\\1406267822.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor(1.),\n",
       " tensor(1.),\n",
       " tensor(1.),\n",
       " tensor(1.),\n",
       " tensor(1.),\n",
       " tensor(1.),\n",
       " tensor(1.),\n",
       " tensor(1.),\n",
       " tensor(1.),\n",
       " tensor(1.),\n",
       " tensor(1.),\n",
       " tensor(1.)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def R_square(A: torch.tensor, B: torch.tensor) -> torch.float32:\n",
    "    assert A.shape == B.shape, \"Predict value not match the Ground Truth\"\n",
    "    # A: predict   B: ground truth\n",
    "    # shape: batch_size * 1 * w * h\n",
    "    A = A.detach()\n",
    "    B = B.detach()\n",
    "    A = A.squeeze()\n",
    "    B = B.squeeze()\n",
    "    # batch_size * w * h\n",
    "    *_, h = A.shape\n",
    "    pre_bar = torch.mean(A, dim=[0,1], keepdim=False)\n",
    "    gt_bar = torch.mean(B, dim=[0,1], keepdim=False)\n",
    "    # print(pre_bar.shape[0])\n",
    "\n",
    "    def sq_sum(x):\n",
    "        # print(x.shape)\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        return torch.sum(x * x, dim=[0,1])\n",
    "    # print(A[:,:,1].shape, pre_bar[1].shape)\n",
    "    SST = [sq_sum(A[:,:,i] - pre_bar[i]) for i in range(h)]\n",
    "    SSR = [sq_sum(B[:,:,i] - gt_bar[i]) for i in range(h)]\n",
    "\n",
    "\n",
    "    return [ (SST[i] / SSR[i]) for i in range(h) ]\n",
    "\n",
    "\"\"\"\n",
    "R-squared = SSR / SST = 1 - SSE / SST\n",
    "\"\"\"\n",
    "A = torch.arange(48.*2).reshape(2,1,4,12)   # test\n",
    "R_square(A, A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/125 [00:00<?, ?it/s]C:\\Users\\Chen Ding\\AppData\\Local\\Temp\\ipykernel_22380\\2525940872.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:01<00:00, 83.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1|1000] 1.51(s) Train_Loss=225.370374 (%)R-squared: [0.03175225108861923, 0.008131429553031921, 0.007069754414260387, 0.4068453013896942, 0.03185199573636055]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:01<00:00, 99.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2|1000] 1.26(s) Train_Loss=207.298831 (%)R-squared: [0.10405036807060242, 0.031928371638059616, 0.016146285459399223, 1.894513726234436, 0.06661615520715714]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:01<00:00, 98.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3|1000] 1.28(s) Train_Loss=185.708618 (%)R-squared: [0.20180198550224304, 0.05431358888745308, 0.02498510479927063, 1.302838921546936, 0.09180133789777756]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:01<00:00, 99.79it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4|1000] 1.26(s) Train_Loss=162.685455 (%)R-squared: [0.15350385010242462, 0.05046546831727028, 0.0912424623966217, 4.729371070861816, 0.20897717773914337]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:01<00:00, 114.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5|1000] 1.10(s) Train_Loss=142.098632 (%)R-squared: [0.27406004071235657, 0.07554352283477783, 0.07886458188295364, 0.4753417372703552, 0.14170411229133606]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 114/125 [00:00<00:00, 118.56it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m train_pred \u001b[39m=\u001b[39m net(use[\u001b[39m0\u001b[39m])    \u001b[39m# use[0].cuda()\u001b[39;00m\n\u001b[0;32m     24\u001b[0m batch_loss \u001b[39m=\u001b[39m Loss(train_pred, use[\u001b[39m1\u001b[39m])   \u001b[39m# use[1].cuda()\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m batch_loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     26\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     27\u001b[0m R2 \u001b[39m=\u001b[39m R_square(train_pred, use[\u001b[39m1\u001b[39m])\n",
      "File \u001b[1;32md:\\Download\\Ana\\envs\\ldm\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32md:\\Download\\Ana\\envs\\ldm\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x_plt, train_loss_plt = [], []\n",
    "global lr, num_epoch, batch_size\n",
    "lr, num_epoch, batch_size = 0.001, 1000, 10\n",
    "Data_Iter = data.DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "net = Try(batch_size=batch_size, seq=15)\n",
    "Loss = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr)\n",
    "scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "loss_list = []\n",
    "\n",
    "\n",
    "print(\"Start Training...\")\n",
    "for epoch in range(num_epoch):\n",
    "    epoch_start_time = time.time()\n",
    "    train_loss = 0.0\n",
    "    net.train()\n",
    "    for i, use in enumerate(tqdm(Data_Iter)):\n",
    "        optimizer.zero_grad()\n",
    "        # if use[0].shape[0]==2:\n",
    "        #     print(use[0])\n",
    "        train_pred = net(use[0])    # use[0].cuda()\n",
    "\n",
    "        batch_loss = Loss(train_pred, use[1])   # use[1].cuda()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        R2 = R_square(train_pred, use[1])\n",
    "\n",
    "        train_loss += batch_loss.item()\n",
    "\n",
    "    train_loss = train_loss / train_size\n",
    "    x_plt.append(epoch+1)\n",
    "    train_loss_plt.append(train_loss)\n",
    "    print(\"[%2d|%2d] %.2f(s) Train_Loss=%.6f (%%)\"%\\\n",
    "            (epoch+1,num_epoch,time.time()-epoch_start_time,train_loss),end='')\n",
    "    print(\"R-squared: \",end='')\n",
    "    R2 = torch.Tensor(R2).tolist()\n",
    "    print(R2)\n",
    "    scheduler.step()\n",
    "\n",
    "plt.figure(1)\n",
    "# plt.plot(x_plt,train_acc_plt,'ob',label='all_train_acc')\n",
    "# plt.figure(2)\n",
    "plt.plot(x_plt,train_loss_plt,'rs-',label='all_train_loss')\n",
    "plt.show()\n",
    "\n",
    "torch.save(net.state_dict(),'./model_cnn.pth')\n",
    "print(\"Parameters Saved.\")  \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ldm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

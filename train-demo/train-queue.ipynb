{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-12T09:42:52.631644Z","iopub.status.busy":"2023-04-12T09:42:52.631009Z","iopub.status.idle":"2023-04-12T09:42:56.312911Z","shell.execute_reply":"2023-04-12T09:42:56.311858Z","shell.execute_reply.started":"2023-04-12T09:42:52.631592Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import torch\n","import torchvision\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","from torch.utils import data\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from torch.utils.data import DataLoader\n","from torch.utils.data import random_split\n","from torch.optim import lr_scheduler\n","from torch.optim import lr_scheduler\n","import time\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","from torch.utils.data import random_split\n","\n","global cr\n","cr = 0.98\n","\n","\n","def normalization(x: list):\n","    M, m = np.max(x), np.min(x)\n","    for i in range(len(x)):\n","        x[i] = (x[i] - (M + m) / 2) / ((M - m) / 2)\n","    # x in [-1, 1]\n","    return M, m, x\n","def cal(x: list):\n","    mu, sigma = 0, 0\n","    for i in range(len(x)):\n","        mu += x[i]\n","    mu /= len(x)*1.0\n","    for i in range(len(x)):\n","        sigma += (x[i]-mu) ** 2\n","    sigma /= len(x)*1.0\n","    from math import sqrt\n","    for i in range(len(x)):\n","        x[i] = (x[i]-mu) / sqrt(sigma)\n","    return mu, sigma, x\n","\n","def ArrNorm(x: np.ndarray, state='max-min'):\n","    assert isinstance(x, np.ndarray), \"We need a list\"\n","    \n","    if state is 'max-min':\n","        M_list, m_list, res = [], [], []\n","        for i in range(x.shape[0]):\n","            u = x[i].tolist()\n","            M, m, t = normalization(u)\n","            res.append(t)\n","            M_list.append(M)\n","            m_list.append(m)\n","        return M_list, m_list, np.array(res)\n","    elif state is 'standard':\n","        mu_list, sigma_list, res = [], [], []\n","        for i in range(x.shape[0]):\n","            u = x[i].tolist()\n","            m, s, t = cal(u)\n","            res.append(t)\n","            mu_list.append(m)\n","            sigma_list.append(s)\n","        return mu_list, sigma_list, res\n","\n","\n","def df2arr(x) -> np.ndarray:\n","    return np.array(x, dtype=np.float32)\n","\n","\n","\n","# one train demo\n","\n","class Try(nn.Module):\n","    def __init__(self, seq, batch_size, scale=0):\n","        super(Try, self).__init__()\n","        self.scale = scale\n","        self.seq = seq\n","        self.batch_size = batch_size\n","        self.linear = nn.Sequential(\n","            nn.Linear((self.seq+1)*12, (self.seq+1)*6),\n","            nn.Dropout(0.5),\n","            nn.Sigmoid(),\n","\n","            nn.Linear((self.seq+1)*6, (self.seq+1)*6),\n","            nn.Dropout(0.5),\n","            nn.ReLU(inplace=True),\n","\n","        )\n","        self.conv1 = nn.Sequential(\n","            # seq * 5 \n","            nn.Conv2d(1, 2, kernel_size=(3,3), padding=2, bias=False), # (seq+2) * 7\n","            nn.BatchNorm2d(2),\n","            nn.ReLU(inplace=True),\n","\n","            nn.Conv2d(2, 2, kernel_size=(3,3), padding=1, bias=False), # (seq+2) * 7\n","            nn.BatchNorm2d(2),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=(2,2), stride=1), # (seq+1) * 6\n","            \n","        )\n","        \n","        self.conv2 = nn.Sequential(\n","            nn.Conv2d(1, 2, kernel_size=(2,2), padding=0, bias=False), # seq * 5\n","            nn.BatchNorm2d(2),\n","            nn.ReLU(inplace=True),\n","\n","            nn.Conv2d(2, 1, kernel_size=(1,1), padding=0, bias=True), # seq * 5\n","            nn.ReLU(inplace=True)\n","\n","        )\n","\n","    def forward(self, x):\n","        out = self.conv1(x)\n","        out = out.view(out.size()[0], -1)\n","        out = self.linear(out)\n","        # print(out.shape)\n","        with torch.no_grad():\n","            out = out.reshape(self.batch_size, 1, self.seq+1, 6)\n","        out = self.conv1(out)\n","        assert out.shape==x.shape, \"Shape Unequal Error.\"\n","        return out + x\n","    \n","    \n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-12T09:42:56.315271Z","iopub.status.busy":"2023-04-12T09:42:56.314800Z","iopub.status.idle":"2023-04-12T09:42:56.320476Z","shell.execute_reply":"2023-04-12T09:42:56.319248Z","shell.execute_reply.started":"2023-04-12T09:42:56.315243Z"},"trusted":true},"outputs":[],"source":["# !pip show torch\n","# !pip show pandas\n","# !pip show matplotlib\n","# !pip show tqdm\n","# !pip show scipy"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-12T09:42:56.322639Z","iopub.status.busy":"2023-04-12T09:42:56.322018Z","iopub.status.idle":"2023-04-12T09:42:56.567959Z","shell.execute_reply":"2023-04-12T09:42:56.566793Z","shell.execute_reply.started":"2023-04-12T09:42:56.322602Z"},"trusted":true},"outputs":[],"source":["import math\n","import numpy as np\n","from scipy import stats\n","def rsquared(x, y): \n","#     print(x.shape)\n","    length = x.shape[-1]\n","    assert x.shape==y.shape, \"Unequal Shape Error\"\n","    r, x, y = [], x.detach(), y.detach()\n","    x, y = x.mean(dim=[0,1], keepdim=False), y.mean(dim=[0,1], keepdim=False)\n","#     print(x.shape)\n","    for i in range(length):\n","        _, _, r_value, _, _ = stats.linregress(x[:,i].detach().numpy(), y[:,i].detach().numpy()) \n","        r.append(r_value ** 2)\n","    return r"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-12T09:42:56.571763Z","iopub.status.busy":"2023-04-12T09:42:56.571163Z","iopub.status.idle":"2023-04-12T09:42:56.586674Z","shell.execute_reply":"2023-04-12T09:42:56.585574Z","shell.execute_reply.started":"2023-04-12T09:42:56.571708Z"},"trusted":true},"outputs":[],"source":["class NET(nn.Module):\n","    def __init__(self, seq, batch_size, ablation_scale=1.0):\n","        super(NET, self).__init__()\n","        self.seq = seq\n","        self.batch_size = batch_size\n","        self.scale = ablation_scale\n","        self.input_size = self.hidden_size = seq\n","        \n","        self.is_directional = False\n","        self.num_direction = 2 if self.is_directional else 1\n","        self.num_layers = 1\n","        \n","        self.lstm = nn.LSTM(self.input_size, self.hidden_size, self.num_layers, dropout=0.5, bidirectional=self.is_directional, batch_first=False)\n","        \n","        self.decouple = nn.Sequential(\n","            nn.Linear(self.seq*5,self.seq*5),  # linear decouple\n","            nn.Dropout(0.5),\n","#             nn.Sigmoid(),   # use sigmoid to enhance unlinear regression may cause data shifting\n","            nn.Linear(self.seq*5, self.seq*5),     # unlinear pool\n","        )\n","        self.conv = nn.Sequential(\n","            # seq * 5 \n","            nn.Conv2d(1, 2, kernel_size=(3,3), padding=1, bias=False), # seq * 5\n","            nn.BatchNorm2d(2),\n","            nn.ReLU(inplace=True),\n","\n","            nn.Conv2d(2, 2, kernel_size=(1,1), padding=0, bias=False),  # seq * 5\n","            nn.BatchNorm2d(2),\n","            nn.ReLU(inplace=True),\n","            \n","            nn.Conv2d(2, 1, kernel_size=(3,3), padding=1, bias=True), # seq * 5\n","            nn.ReLU(inplace=True),\n","        )\n","    \n","    def reset(self, scale=1.0):\n","        self.ablatiion_scale = scale\n","\n","    def forward(self, x):\n","        x = x.squeeze()\n","        \n","        h_0 = torch.randn(self.num_direction*self.num_layers, self.seq, self.hidden_size).cuda()\n","        c_0 = torch.randn(self.num_direction*self.num_layers, self.seq, self.hidden_size).cuda()\n","        pred, *_ = self.lstm(x, (h_0, c_0))\n","        pred = pred.flatten(1)\n","        out1 = self.decouple(pred)\n","        with torch.no_grad():\n","            out1 = out1.reshape(-1, 1, self.seq, 5)\n","        out2 = self.conv(out1)\n","        assert out1.shape==out2.shape, \"Shape Unequal Error.\"\n","        return self.scale * out1 + out2"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-12T09:42:56.591832Z","iopub.status.busy":"2023-04-12T09:42:56.590864Z","iopub.status.idle":"2023-04-12T09:42:57.780939Z","shell.execute_reply":"2023-04-12T09:42:57.779715Z","shell.execute_reply.started":"2023-04-12T09:42:56.591788Z"},"trusted":true},"outputs":[],"source":["excel = pd.read_excel('/kaggle/input/a32-data/A32.xlsx', header=None)\n","excel.shape\n","\n","sp = [1486, 2972, 4458]\n","station_1 = excel.iloc[1:sp[0]+1,1:6]\n","station_2 = excel.iloc[sp[0]+1:sp[1]+1,1:6]\n","standard = excel.iloc[sp[1]+1:sp[2]+1,1:6]\n","\n","station_1 = df2arr(station_1)\n","station_2 = df2arr(station_2)\n","standard = df2arr(standard)\n","station_1.shape, station_2.shape, standard.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-12T09:42:57.783217Z","iopub.status.busy":"2023-04-12T09:42:57.782647Z","iopub.status.idle":"2023-04-12T09:42:57.793181Z","shell.execute_reply":"2023-04-12T09:42:57.791916Z","shell.execute_reply.started":"2023-04-12T09:42:57.783172Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["s1_minus_sd = station_1 - standard\n","s2_minus_sd = station_2 - standard\n","s1_div_sd = station_1 / standard\n","s2_div_sd = station_2 / standard\n","\n","s1_minus_sd.shape, s2_minus_sd.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-12T09:42:57.795979Z","iopub.status.busy":"2023-04-12T09:42:57.795297Z","iopub.status.idle":"2023-04-12T09:42:57.821503Z","shell.execute_reply":"2023-04-12T09:42:57.820321Z","shell.execute_reply.started":"2023-04-12T09:42:57.795939Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["s1_M, s1_m, s1 = ArrNorm(station_1, 'standard')\n","s2_M, s2_m, s2 = ArrNorm(station_2, 'standard')\n","\n","import math\n","import numpy as np\n","from scipy import stats\n","def rsquared(x, y): \n","    _, _, r_value, _, _ = stats.linregress(x.detach().cpu().numpy(), y.detach().cpu().numpy()) \n","    return r_value**2"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-12T09:42:57.823567Z","iopub.status.busy":"2023-04-12T09:42:57.823168Z","iopub.status.idle":"2023-04-12T09:42:57.837022Z","shell.execute_reply":"2023-04-12T09:42:57.835821Z","shell.execute_reply.started":"2023-04-12T09:42:57.823489Z"},"trusted":true},"outputs":[],"source":["def GetDataset(input_arr: list, output_arr: list, seq: int):\n","    assert(len(input_arr)==len(output_arr)), \"Different size of input and output!\"\n","    Input = []\n","    Output = []\n","    for i in range(len(input_arr)-seq):\n","        Input.append(input_arr[i:i+seq][:])\n","        Output.append(output_arr[i:i+seq][:])\n","    return torch.tensor(Input, dtype=torch.float32), torch.tensor(Output, dtype=torch.float32)\n","\n","        \n","def load_array(data_arrays, batch_size, is_train=True):\n","    # data-iter\n","    dataset = data.TensorDataset(*data_arrays)\n","    return data.DataLoader(dataset, batch_size, shuffle=is_train)\n","\n","s1_minus_sd.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-12T09:42:57.839095Z","iopub.status.busy":"2023-04-12T09:42:57.838655Z","iopub.status.idle":"2023-04-12T09:43:09.691845Z","shell.execute_reply":"2023-04-12T09:43:09.690559Z","shell.execute_reply.started":"2023-04-12T09:42:57.839059Z"},"trusted":true},"outputs":[],"source":["!pip install loguru"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-12T09:43:09.696884Z","iopub.status.busy":"2023-04-12T09:43:09.696523Z","iopub.status.idle":"2023-04-12T09:43:09.753498Z","shell.execute_reply":"2023-04-12T09:43:09.752533Z","shell.execute_reply.started":"2023-04-12T09:43:09.696850Z"},"trusted":true},"outputs":[],"source":["# global sequence, batch_size\n","# batch_size = 4\n","# sequence = 6\n","\n","# global lr, num_epoch\n","# lr, num_epoch = 0.000001, 100\n","# status_list = ['MINUS', 'DIVIDE']\n","\n","from loguru import logger\n","\n","\n","status_list = [\"MINUS\"]\n","name_list = [\"TRY\", \"NET\"]\n","global record\n","record_R = []\n","record_con = []\n","weights = []\n","cnt = 0\n","\n","\n","with open(r'/kaggle/working/log.txt', 'w', encoding='utf-8') as f:\n","    f.write('START TRAINING...\\n')\n","\n","\n","FILE = open('/kaggle/working/log.txt', \"w\", encoding='utf-8')\n","\n","def train_test(name, status, batch_size, sequence, lr, num_epoch, ga, scale):\n","    global cnt\n","    cnt += 1\n","    training_scale_ = scale\n","    string = \"&sequence=\" + str(sequence) + \"&scale=\" + str(scale) \n","    weights.append(string)\n","    \n","    Input_Data_1, Output_Data_1 = GetDataset(s1, s1_minus_sd, sequence) if status==\"MINUS\" else GetDataset(s1,s1_div_sd, sequence)\n","    Input_Data_2, Output_Data_2 = GetDataset(s2, s2_minus_sd, sequence) if status==\"MINUS\" else GetDataset(s2,s2_div_sd, sequence)\n","    Input_Data_1.shape, Input_Data_2.shape\n","\n","    Input_Data_1 = Input_Data_1.unsqueeze(1)\n","    Output_Data_1 = Output_Data_1.unsqueeze(1)\n","    Input_Data_2 = Input_Data_2.unsqueeze(1)\n","    Output_Data_2 = Output_Data_2.unsqueeze(1)\n","    Input_Data_1.shape, Input_Data_1.shape\n","\n","\n","    data_tot_1 = torch.utils.data.TensorDataset(Input_Data_1, Output_Data_1)\n","    data_tot_2 = torch.utils.data.TensorDataset(Input_Data_2, Output_Data_2)\n","    train_size = int(Input_Data_1.shape[0] * cr)\n","    test_size = Input_Data_1.shape[0] - train_size\n","    train_set_1, test_set_1 = random_split(data_tot_1,[train_size,test_size],\n","                                        torch.Generator().manual_seed(0))\n","    train_set_2, test_set_2 = random_split(data_tot_2,[train_size,test_size],\n","                                        torch.Generator().manual_seed(0))\n","\n","\n","\n","\n","    Data_Iter_1 = DataLoader(dataset=train_set_1, batch_size=batch_size, shuffle=True, drop_last=True)\n","    Data_Iter_2 = DataLoader(dataset=train_set_2, batch_size=batch_size, shuffle=True, drop_last=True)\n","\n","    x_plt, train_loss_plt = [], []\n","\n","\n","    net = Try(batch_size=batch_size, seq=sequence).cuda() if name==\"TRY\" else NET(batch_size=batch_size, seq=sequence, ablation_scale=scale).cuda()\n","    Loss = torch.nn.MSELoss()\n","    optimizer = torch.optim.Adam(net.parameters(), lr)\n","    scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=ga)\n","\n","    def Iter(num_epoch):\n","        cnt = 0\n","        while cnt < num_epoch:\n","            yield cnt\n","            cnt += 1\n","\n","    print(\"\\nStart Training with condition: \"+string)\n","    print(\"Round 1...\")\n","    for epoch in range(num_epoch):\n","        epoch_start_time = time.time()\n","        train_loss = 0.0\n","        net.train()\n","        for i, use in enumerate(tqdm(Data_Iter_1)):\n","            optimizer.zero_grad()\n","            train_pred = net(use[0].cuda())    # use[0].cuda()\n","\n","            batch_loss = Loss(train_pred, use[1].cuda())   # use[1].cuda()\n","            batch_loss.backward()\n","            optimizer.step()\n","\n","            train_loss += batch_loss.item()\n","\n","        train_loss = train_loss / train_size\n","        x_plt.append(epoch+1)\n","        train_loss_plt.append(train_loss)\n","        print(\"Round-1 --- [%2d|%2d] %.2f(s) Train_Loss=%.6f \"%\\\n","                (epoch+1,num_epoch,time.time()-epoch_start_time,train_loss),end='')\n","        scheduler.step()  \n","\n","    plt.figure(1)\n","    plt.plot(x_plt,train_loss_plt,'rs-',label='all_train_loss')\n","    plt.show()\n","\n","    print(\"Round 2...\")\n","    for epoch in range(num_epoch):\n","        epoch_start_time = time.time()\n","        train_loss = 0.0\n","        net.train()\n","        for i, use in enumerate(tqdm(Data_Iter_2)):\n","            optimizer.zero_grad()\n","            train_pred = net(use[0].cuda())    # use[0].cuda()\n","\n","            batch_loss = Loss(train_pred, use[1].cuda())   # use[1].cuda()\n","            batch_loss.backward()\n","            optimizer.step()\n","\n","            train_loss += batch_loss.item()\n","\n","        train_loss = train_loss / train_size\n","        x_plt.append(epoch+num_epoch+1)\n","        train_loss_plt.append(train_loss)\n","        print(\"Round 2 --- [%2d|%2d] %.2f(s) Train_Loss=%.6f \"%\\\n","                (epoch+1,num_epoch,time.time()-epoch_start_time,train_loss),end='')\n","        scheduler.step() \n","\n","\n","    plt.figure(2)\n","    plt.plot(x_plt,train_loss_plt,'rs-',label='all_train_loss')\n","    plt.show()\n","\n","    torch.save(net.state_dict(), '/kaggle/working/'+string+\".pt\")\n","    print(\"Parameters Saved.\")\n","\n","\n","    Test_Iter_1 = DataLoader(dataset=test_set_1, batch_size=len(test_set_1), shuffle=False, drop_last=True)\n","    Test_Iter_2 = DataLoader(dataset=test_set_2, batch_size=len(test_set_2), shuffle=False, drop_last=True)\n","    model = NET(batch_size=1, seq=sequence, ablation_scale=0.5)\n","    model.load_state_dict(torch.load('/kaggle/working/'+string+'.pt'))\n","    model = model.cuda()\n","    model.eval()\n","    \n","    \n","    \n","    scale = 0.5\n","    model.reset(scale)\n","    R_1, R_2 = [], []\n","    with torch.no_grad():\n","        for i, use in enumerate(Test_Iter_1):\n","            pred = model(use[0].cuda())\n","            pred = pred.reshape(-1, 5)\n","            use[1] = use[1].reshape(-1, 5)\n","            assert pred.shape == use[1].shape, 'unequal shape error'\n","            for i in range(pred.shape[1]):\n","                R_1.append(rsquared(pred[i], use[1][i]))\n","            assert len(R_1) == 5, 'Err'\n","        for i, use in enumerate(Test_Iter_2):\n","            pred = model(use[0].cuda())\n","            pred = pred.reshape(-1, 5)\n","            use[1] = use[1].reshape(-1, 5)\n","            assert pred.shape == use[1].shape, 'unequal shape error'\n","            for i in range(pred.shape[1]):\n","                R_2.append(rsquared(pred[i], use[1][i]))\n","            assert len(R_2) == 5, 'Err'\n","            \n","            print('Guidance Scale = {0}, R_1 = {1}, R_2 = {2}'.format(scale, R_1, R_2))\n","    \n","    def find_scale(idx: int):\n","        # feature_index: idx\n","        model = NET(batch_size=1, seq=sequence, ablation_scale=0.5)\n","        model.load_state_dict(torch.load('/kaggle/working/'+string+'.pt'))\n","        model = model.cuda()\n","        model.eval()\n","\n","        Test_Iter_1 = DataLoader(dataset=test_set_1, batch_size=len(test_set_1), shuffle=False, drop_last=True)\n","        Test_Iter_2 = DataLoader(dataset=test_set_2, batch_size=len(test_set_2), shuffle=False, drop_last=True)\n","\n","        r = .0\n","        r12 = []\n","        best_scale = .0\n","\n","        for scale in np.arange(.0, 10.0, 0.1):\n","            model.reset(scale)\n","            with torch.no_grad():\n","                for i, use in enumerate(Test_Iter_1):\n","                    pred = model(use[0].cuda())\n","                    pred = pred.reshape(-1, 5)\n","                    use[1] = use[1].reshape(-1, 5)\n","                    assert pred.shape == use[1].shape, 'unequal shape error'\n","                    r1 = rsquared(pred[idx], use[1][idx])\n","                for i, use in enumerate(Test_Iter_2):\n","                    pred = model(use[0].cuda())\n","                    pred = pred.reshape(-1, 5)\n","                    use[1] = use[1].reshape(-1, 5)\n","                    assert pred.shape == use[1].shape, 'unequal shape error'\n","                    r2 = rsquared(pred[idx], use[1][idx])\n","\n","                r_ = r1 + r2\n","                if r_ >= r:\n","                    r = r_\n","                    best_scale = scale\n","                    r12 = [r1, r2]\n","        return r, best_scale, r12\n","        \n","    r = []\n","    best_scale = []\n","    r12 = []\n","    for i in range(5):\n","        r_, scale_, r12_ = find_scale(i)\n","        r.append(r_ / 2)\n","        r12.append(r12_)\n","        best_scale.append(scale_)\n","        \n","    logger.info('training scale: ')\n","    logger.info(training_scale_)\n","    logger.info('R List: ')\n","    logger.info(r)\n","    logger.info('Best Scale List: ')\n","    logger.info(best_scale)\n","    logger.info('\\n')\n","    \n","    print('training scale:\\n', training_scale_)\n","    print('R List:\\n', r)\n","    print('Best Scale List:\\n', best_scale)\n","    print('Two of R in List:\\n', r12)\n","    \n","    FILE.write('training scale: ')\n","    FILE.write('\\n')\n","    FILE.write(str(training_scale_))\n","    FILE.write('\\n')\n","    FILE.write('R List: ')\n","    FILE.write('\\n')\n","    FILE.write(str(r))\n","    FILE.write('\\n')\n","    FILE.write('Best Scale List: ')\n","    FILE.write('\\n')\n","    FILE.write(str(best_scale))\n","    FILE.write('\\n')\n","    FILE.write('Two of R in List: ')\n","    FILE.write('\\n')\n","    FILE.write(str(r12))\n","    FILE.write('\\n')\n","    FILE.write('-'*90)\n","    FILE.write('\\n')\n","    \n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-12T09:43:09.755295Z","iopub.status.busy":"2023-04-12T09:43:09.754864Z"},"trusted":true},"outputs":[],"source":["name_list = [\"NET\"]\n","status = \"MINUS\"\n","num_epoch = 3000\n","ga = 0.90\n","lr = 0.0003\n","\n","for train_scale in np.arange(-10., 10., .5):\n","    for batch_size in [16]:\n","        for sequence in [5]:\n","            for name in name_list:\n","                train_test(name, status, batch_size, sequence, lr, num_epoch, ga, train_scale)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["FILE.close()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
